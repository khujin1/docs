{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"1. LightningDB is \u00b6 A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data accesses based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data 2. Architecture \u00b6 Spark with Redis/Rocksdb key value stores No I/O bottleneck due to redis in DRAM and rocksdb in SSDs due to the small sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned 3. Features \u00b6 Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering \u2022High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD life time","title":"Introduction"},{"location":"#1-lightningdb-is","text":"A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data accesses based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data","title":"1. LightningDB is"},{"location":"#2-architecture","text":"Spark with Redis/Rocksdb key value stores No I/O bottleneck due to redis in DRAM and rocksdb in SSDs due to the small sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned","title":"2. Architecture"},{"location":"#3-features","text":"Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering \u2022High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD life time","title":"3. Features"},{"location":"FAQ/","text":"Q1: LightningDB is free? A1: Yes, free license. Q2: Is there any presentation experience in major conference? A2: Yes, in Spark AI Summit Europe 2019 Apache Spark AI Use Case in Telco: Network Quality Analysis and Prediction","title":"FAQ"},{"location":"Support/","text":"Commercial Support \u00b6 ... Trouble-shooting \u00b6 ...","title":"Commercial Support"},{"location":"Support/#commercial-support","text":"...","title":"Commercial Support"},{"location":"Support/#trouble-shooting","text":"...","title":"Trouble-shooting"},{"location":"command-line-interface/","text":"1. cluster start \u00b6 Procedure (1) Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in '${SR2_HOME} 1 /logs/redis/' will be moved to '${SR2_HOME}/logs/redis/backup/'. (2) Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in '${SR2_REDIS_DATA}' (3) Start redis-server process Start master and slave redis-server with '${SR2_HOME}/conf/redis/redis-{port}.conf' file Log files will be saved in '${SR2_HOME}/logs/redis/' > cluster start Check status of hosts ... OK Check cluster exist... ... OK Backup redis master log in each MASTER hosts... ... Starting master nodes : nodeA : 18100 ... ... Starting slave nodes : nodeA : 18150|18151 ... ... Wait until all redis process up... cur: 0 / total: 12 ... cur: 12 / total: 12 Complete all redis process up. Error Handling (1) ErrorCode 11 Redis-server(master) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. (2) ErrorCode 12 Redis-server(slave) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. (3) Conf file not exist Conf file is not found. To resove this error, use 'cluster configure' and then 'cluster start'. cluster configure \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\uc2dc\ud0a8 \ud6c4 cluster start\ub97c \uc9c4\ud589\ud558\uc138\uc694. $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf (4) max try error \u200b For detail information, please check log files. $ cluster start ... max try error 2. cluster stop \u00b6 \u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b Options (1) Force to kill all redis-servers(master/slave) with SIGKILL --force 3. cluster create \u00b6 After checking information of the cluster, create cluster of LightningDB. > cluster create >>> Creating cluster +-------+-------+--------+ | HOST | PORT | TYPE | +-------+-------+--------+ | nodeA | 18100 | MASTER | | nodeB | 18100 | MASTER | | . . . | | . . . | | . . . | | nodeD | 18150 | SLAVE | | nodeD | 18151 | SLAVE | +-------+-------+--------+ Do you want to proceed with the create according to the above information? (y/n) y replicas: 2.00 replicate [M] nodeA 18100 - [S] nodeA 18150 replicate [M] nodeD 18100 - [S] nodeD 18151 1 / 8 meet complete. 2 / 8 meet complete. ... 8 / 8 meet complete. create cluster complete. \u200b\u200b 4. cluster clean \u00b6 Procedure\u200b (1) Remove conf files for redis-server (2) Remove all data(aof, rdb, RocksDB) of LightningDB 5. cluster restart\u200b \u00b6 Process 'cluster stop' and then 'cluster start'.\u200b\u200b Options (1) Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop (2) Remove all data(aof, rdb, RocksDB, conf files) before start again. --reset (3) Process 'cluster create'. This command should be called with '--reset'. --cluster 6. Check Cluster Infos \u00b6 cli ping --all Send PING to all redis-server processes and check if all of them are healthy. cli cluster info Check information of the cluster. cli cluster nodes List up all redis-server processes that compose the cluster. \u200b \u200b If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9","title":"Commands"},{"location":"command-line-interface/#1-cluster-start","text":"Procedure (1) Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in '${SR2_HOME} 1 /logs/redis/' will be moved to '${SR2_HOME}/logs/redis/backup/'. (2) Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in '${SR2_REDIS_DATA}' (3) Start redis-server process Start master and slave redis-server with '${SR2_HOME}/conf/redis/redis-{port}.conf' file Log files will be saved in '${SR2_HOME}/logs/redis/' > cluster start Check status of hosts ... OK Check cluster exist... ... OK Backup redis master log in each MASTER hosts... ... Starting master nodes : nodeA : 18100 ... ... Starting slave nodes : nodeA : 18150|18151 ... ... Wait until all redis process up... cur: 0 / total: 12 ... cur: 12 / total: 12 Complete all redis process up. Error Handling (1) ErrorCode 11 Redis-server(master) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. (2) ErrorCode 12 Redis-server(slave) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. (3) Conf file not exist Conf file is not found. To resove this error, use 'cluster configure' and then 'cluster start'. cluster configure \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\uc2dc\ud0a8 \ud6c4 cluster start\ub97c \uc9c4\ud589\ud558\uc138\uc694. $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf (4) max try error \u200b For detail information, please check log files. $ cluster start ... max try error","title":"1. cluster start"},{"location":"command-line-interface/#2-cluster-stop","text":"\u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b Options (1) Force to kill all redis-servers(master/slave) with SIGKILL --force","title":"2. cluster stop"},{"location":"command-line-interface/#3-cluster-create","text":"After checking information of the cluster, create cluster of LightningDB. > cluster create >>> Creating cluster +-------+-------+--------+ | HOST | PORT | TYPE | +-------+-------+--------+ | nodeA | 18100 | MASTER | | nodeB | 18100 | MASTER | | . . . | | . . . | | . . . | | nodeD | 18150 | SLAVE | | nodeD | 18151 | SLAVE | +-------+-------+--------+ Do you want to proceed with the create according to the above information? (y/n) y replicas: 2.00 replicate [M] nodeA 18100 - [S] nodeA 18150 replicate [M] nodeD 18100 - [S] nodeD 18151 1 / 8 meet complete. 2 / 8 meet complete. ... 8 / 8 meet complete. create cluster complete. \u200b\u200b","title":"3. cluster create"},{"location":"command-line-interface/#4-cluster-clean","text":"Procedure\u200b (1) Remove conf files for redis-server (2) Remove all data(aof, rdb, RocksDB) of LightningDB","title":"4. cluster clean"},{"location":"command-line-interface/#5-cluster-restart","text":"Process 'cluster stop' and then 'cluster start'.\u200b\u200b Options (1) Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop (2) Remove all data(aof, rdb, RocksDB, conf files) before start again. --reset (3) Process 'cluster create'. This command should be called with '--reset'. --cluster","title":"5. cluster restart\u200b"},{"location":"command-line-interface/#6-check-cluster-infos","text":"cli ping --all Send PING to all redis-server processes and check if all of them are healthy. cli cluster info Check information of the cluster. cli cluster nodes List up all redis-server processes that compose the cluster. \u200b \u200b If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9","title":"6. Check Cluster Infos"},{"location":"data-ingestion-and-querying/","text":"1. Table Reads & Writes \u00b6 Create a table \u00b6 You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". df.write.format(\"r2\").insertInto(r2TableName) 2. Data Ingestion and Querying \u00b6 3. Querying \u00b6","title":"Data Ingestion and Querying"},{"location":"data-ingestion-and-querying/#1-table-reads-writes","text":"","title":"1. Table Reads &amp; Writes"},{"location":"data-ingestion-and-querying/#create-a-table","text":"You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". df.write.format(\"r2\").insertInto(r2TableName)","title":"Create a table"},{"location":"data-ingestion-and-querying/#2-data-ingestion-and-querying","text":"","title":"2. Data Ingestion and Querying"},{"location":"data-ingestion-and-querying/#3-querying","text":"","title":"3. Querying"},{"location":"eula/","text":"LightningDB - EULA \u00b6 End\u00adUser License Agreement (\"Agreement\") \u00b6 Last updated: (Sep. 26. 2019) Please read this End\u00adUser License Agreement (\"Agreement\") carefully before clicking the \"Subscribing\" button, or using LightningDB\u200b(\"Application\"). By clicking the \"Subscribe\" button, or using the Application, you are agreeing to be bound by the terms and conditions of this Agreement. If you do not agree to the terms of this Agreement, do not click on the \"Subscribe\" button and do not subscribe or use the Application. License \u00b6 SK Telecom \u200bgrants you a revocable, non\u00adexclusive, non\u00adtransferable, limited license to subscribe and use the Application solely for your personal, non\u00adcommercial purposes strictly in accordance with the terms of this Agreement. Restrictions \u00b6 You agree not to, and you will not permit others to: a) license, sell, rent, lease, assign, distribute, transmit, host, outsource, disclose or otherwise commercially exploit the Application or make the Application available to any third party. Modifications to Application \u00b6 SK Telecom \u200breserves the right to modify, suspend or discontinue, temporarily or permanently, the Application or any service to which it connects, with or without notice and without liability to you. Term and Termination \u00b6 This Agreement shall remain in effect until terminated by you or SK Telecom.\u200b SK Telecom \u200bmay, in its sole discretion, at any time and for any or no reason, suspend or terminate this Agreement with or without prior notice. This Agreement will terminate immediately, without prior notice from SK Telecom,\u200bin the event that you fail to comply with any provision of this Agreement. You may also terminate this Agreement by unsubscribing the Application. Upon termination of this Agreement, you shall cease all use of the Application and delete all copies of the Application from your device or from your cloud. Severability \u00b6 If any provision of this Agreement is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect. Amendments to this Agreement \u00b6 SK Telecom \u200breserves the right, at its sole discretion, to modify or replace this Agreement at any time. If a revision is material we will provide at least 7 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion. Contact Information \u00b6 If you have any questions about this Agreement, please contact us.","title":"LightningDB - EULA"},{"location":"eula/#lightningdb-eula","text":"","title":"LightningDB - EULA"},{"location":"eula/#enduser-license-agreement-agreement","text":"Last updated: (Sep. 26. 2019) Please read this End\u00adUser License Agreement (\"Agreement\") carefully before clicking the \"Subscribing\" button, or using LightningDB\u200b(\"Application\"). By clicking the \"Subscribe\" button, or using the Application, you are agreeing to be bound by the terms and conditions of this Agreement. If you do not agree to the terms of this Agreement, do not click on the \"Subscribe\" button and do not subscribe or use the Application.","title":"End\u00adUser License Agreement (\"Agreement\")"},{"location":"eula/#license","text":"SK Telecom \u200bgrants you a revocable, non\u00adexclusive, non\u00adtransferable, limited license to subscribe and use the Application solely for your personal, non\u00adcommercial purposes strictly in accordance with the terms of this Agreement.","title":"License"},{"location":"eula/#restrictions","text":"You agree not to, and you will not permit others to: a) license, sell, rent, lease, assign, distribute, transmit, host, outsource, disclose or otherwise commercially exploit the Application or make the Application available to any third party.","title":"Restrictions"},{"location":"eula/#modifications-to-application","text":"SK Telecom \u200breserves the right to modify, suspend or discontinue, temporarily or permanently, the Application or any service to which it connects, with or without notice and without liability to you.","title":"Modifications to Application"},{"location":"eula/#term-and-termination","text":"This Agreement shall remain in effect until terminated by you or SK Telecom.\u200b SK Telecom \u200bmay, in its sole discretion, at any time and for any or no reason, suspend or terminate this Agreement with or without prior notice. This Agreement will terminate immediately, without prior notice from SK Telecom,\u200bin the event that you fail to comply with any provision of this Agreement. You may also terminate this Agreement by unsubscribing the Application. Upon termination of this Agreement, you shall cease all use of the Application and delete all copies of the Application from your device or from your cloud.","title":"Term and Termination"},{"location":"eula/#severability","text":"If any provision of this Agreement is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.","title":"Severability"},{"location":"eula/#amendments-to-this-agreement","text":"SK Telecom \u200breserves the right, at its sole discretion, to modify or replace this Agreement at any time. If a revision is material we will provide at least 7 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.","title":"Amendments to this Agreement"},{"location":"eula/#contact-information","text":"If you have any questions about this Agreement, please contact us.","title":"Contact Information"},{"location":"get-started-with-aws/","text":"Note This page guides how to start LightningDB automatically only for the case of AWS EC2 Instance . 1. Access EC2 Instance \u00b6 Create EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS} 2. Script for setup environment \u00b6 After access EC2 Instance, run script to setup environment. $ cd ~/flashbase/scripts/userdata/per-boot $ ./run.sh After 'run.sh' is completed, following jobs are done. Create and exchange SSH KEY for user authentication Mount disks Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration 3. Start LightningDB \u00b6 LightningDB provides fbctl that is introduced in Install and Upgrade . With fbctl , user can deploy and use LightningDB. LightningDB supports Zeppelin to provide convenience of ingestion and querying data of LightningDB. About Zeppelin , Data Ingestion and Querying page provides some examples.","title":"Automated Installation (recommended)"},{"location":"get-started-with-aws/#1-access-ec2-instance","text":"Create EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS}","title":"1. Access EC2 Instance"},{"location":"get-started-with-aws/#2-script-for-setup-environment","text":"After access EC2 Instance, run script to setup environment. $ cd ~/flashbase/scripts/userdata/per-boot $ ./run.sh After 'run.sh' is completed, following jobs are done. Create and exchange SSH KEY for user authentication Mount disks Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration","title":"2. Script for setup environment"},{"location":"get-started-with-aws/#3-start-lightningdb","text":"LightningDB provides fbctl that is introduced in Install and Upgrade . With fbctl , user can deploy and use LightningDB. LightningDB supports Zeppelin to provide convenience of ingestion and querying data of LightningDB. About Zeppelin , Data Ingestion and Querying page provides some examples.","title":"3. Start LightningDB"},{"location":"get-started-with-scratch/","text":"Note This page guides how to start LightningDB on CentOS manually. In case of using AWS EC2 Instance , please use 1. Automated Installation (recommanded) 1. Optimizing System Parameters \u00b6 (1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages 2. Setup Prerequisites \u00b6 bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install go to the website: https://software.intel.com/en-us/mkl/choose-download/macos register and login select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks. 3. Session configuration files \u00b6 Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster' 4. Install and Start LightningDB \u00b6 With fbctl provided by LightningDB, user can deploy and use LightningDB. Install fbctl with following command. $ pip insatll fbctl After installation is completed, start fbctl with Commands","title":"Manual Installation"},{"location":"get-started-with-scratch/#1-optimizing-system-parameters","text":"(1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages","title":"1. Optimizing System Parameters"},{"location":"get-started-with-scratch/#2-setup-prerequisites","text":"bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install go to the website: https://software.intel.com/en-us/mkl/choose-download/macos register and login select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks.","title":"2. Setup Prerequisites"},{"location":"get-started-with-scratch/#3-session-configuration-files","text":"Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster'","title":"3. Session configuration files"},{"location":"get-started-with-scratch/#4-install-and-start-lightningdb","text":"With fbctl provided by LightningDB, user can deploy and use LightningDB. Install fbctl with following command. $ pip insatll fbctl After installation is completed, start fbctl with Commands","title":"4. Install and Start LightningDB"},{"location":"release-note/","text":"1. Recommended Versions \u00b6 LightningDB ver 1.0 2. Release Notes \u00b6 Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions","title":"Release Notes"},{"location":"release-note/#1-recommended-versions","text":"LightningDB ver 1.0","title":"1. Recommended Versions"},{"location":"release-note/#2-release-notes","text":"Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions","title":"2. Release Notes"},{"location":"run-and-deploy-fbctl/","text":"Note Command Line Interface(CLI) of LightningDB supports not only deploy and start command but also many commands to access and manipulate data in LightningDB. \u200b 1. How to run fbctl \u00b6 (1) Run \u00b6 To run fbctl, ${FBPATH} should be set. If not, following error messages will be shown. To start using fbctl, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In case of EC2 Instance, this path is set automatically. Run fbctl by typing 'fbctl' $ fbcli When fbctl starts at the first time, user needs to confirm 'base_directory'. [~/tsr2] 1 ] is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', user can modify 'base_directory'. If user logs in fbctl normally, fbctl starts on last visited cluster. In case of first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that user is in Centos and '>' means that user is in fbctl. (2) Log messages \u00b6 Log messages of fbctl will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceed of size limit. 2. Deploy LightningDB \u00b6 Deploy is the procedure that LightningDB is installed with the specified cluster number. User could make LightningDB cluster with the following command. > deploy 1 After deploy command, user should type the following information that provides its last used value. installer host number of masters replicas number of ssd(disk) prefix of (redis data / redis db path / flash db path) Use below option not to save last used value. > deploy --history-save=False (1) Select installer \u00b6 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin With only URL, instead of file path, LightningDB can be installed like below. https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.dbcb9e.bin Downloading flashbase.dev.master.dbcb9e.bin [======= ] 15% (2) Type Hosts \u00b6 IP address or hostname can be used. In case of several hosts, list can be seperated by comma(','). Please type host list separated by comma(,) [127.0.0.1] nodeA, nodeB, nodeC, nodeD OK, ['nodeA', 'nodeB', 'nodeC', 'nodeD'] (3) Type Masters \u00b6 How many masters would you like to create on each host? [1] 1 OK, 1 Please type ports separate with comma(,) and use hyphen(-) for range. [18100] 18100 OK, ['18100'] Define how many master processes will be created in the cluster per server. (4) Type information of slave \u00b6 How many replicas would you like to create on each master? [2] 2 OK, 2 Please type ports separate with comma(,) and use hyphen(-) for range. [18150-18151] 18150-18151 OK, ['18150-18151'] Define how many slave processes will be created for a master process. (5) Type the count of SSD(disk) and the path of DB files \u00b6 How many sdd would you like to use? [3] 3 OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ (6) Check all settings finally \u00b6 Finally all settings will be shown and confirmation will be requested like below. +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y (7) Deploy cluster \u00b6 After deploying is completed, following messages are shown and fbctl of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLEAN | | nodeD | CLEAN | +-----------+--------+ OK Transfer install and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1 Cluster 1 selected. When an error occurs during deploying, error messages will be shown like below. Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If localhost(127.0.0.1) is not included in host information, this error occurs. Please add localhost in host list in this case. From now, user can start and manage clusters of LightningDB with Commands . 3. LightningDB Version Update \u00b6 In case of version update, 'deploy' command is used. > c 1 // alias of 'cluster use 1' > deploy (Watch out) Cluster 1 is already deployed. Do you want to deploy again? (y/n) [n] y (1) Select installer \u00b6 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin (2) Restore \u00b6 Do you want to restore conf? (y/n) y If the current settings will be reused, type 'y'. (3) Check all settings finally \u00b6 +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | redis data path | ~/sata_ssd/ssd_ | | redis db path | ~/sata_ssd/ssd_ | | flash db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ Checking for cluster exist... +------+--------+ | HOST | STATUS | +------+--------+ Backup conf of cluster 1... OK, cluster_1_conf_bak_<time-stamp> Backup info of cluster 1 at nodeA... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeB... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeC... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeD... OK, cluster_1_bak_<time-stamp> Transfer installer and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1. Cluster 1 selected. Backup path of cluster: ${base-directory}/backup/cluster_${cluster-id}_bak_${time-stamp} Backup path of conf files: $FBAPTH/conf_backup/cluster_${cluster-id}_conf_bak_${time-stamp} (4) Restart \u00b6 > cluster restart After restart, new version will be applied. If user types 'enter' without any text, the default value is applied. In some case, default value will not provided. \u21a9","title":"Install and Upgrade"},{"location":"run-and-deploy-fbctl/#1-how-to-run-fbctl","text":"","title":"1. How to run fbctl"},{"location":"run-and-deploy-fbctl/#1-run","text":"To run fbctl, ${FBPATH} should be set. If not, following error messages will be shown. To start using fbctl, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In case of EC2 Instance, this path is set automatically. Run fbctl by typing 'fbctl' $ fbcli When fbctl starts at the first time, user needs to confirm 'base_directory'. [~/tsr2] 1 ] is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', user can modify 'base_directory'. If user logs in fbctl normally, fbctl starts on last visited cluster. In case of first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that user is in Centos and '>' means that user is in fbctl.","title":"(1) Run"},{"location":"run-and-deploy-fbctl/#2-log-messages","text":"Log messages of fbctl will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceed of size limit.","title":"(2) Log messages"},{"location":"run-and-deploy-fbctl/#2-deploy-lightningdb","text":"Deploy is the procedure that LightningDB is installed with the specified cluster number. User could make LightningDB cluster with the following command. > deploy 1 After deploy command, user should type the following information that provides its last used value. installer host number of masters replicas number of ssd(disk) prefix of (redis data / redis db path / flash db path) Use below option not to save last used value. > deploy --history-save=False","title":"2. Deploy LightningDB"},{"location":"run-and-deploy-fbctl/#1-select-installer","text":"Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin With only URL, instead of file path, LightningDB can be installed like below. https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.dbcb9e.bin Downloading flashbase.dev.master.dbcb9e.bin [======= ] 15%","title":"(1) Select installer"},{"location":"run-and-deploy-fbctl/#2-type-hosts","text":"IP address or hostname can be used. In case of several hosts, list can be seperated by comma(','). Please type host list separated by comma(,) [127.0.0.1] nodeA, nodeB, nodeC, nodeD OK, ['nodeA', 'nodeB', 'nodeC', 'nodeD']","title":"(2) Type Hosts"},{"location":"run-and-deploy-fbctl/#3-type-masters","text":"How many masters would you like to create on each host? [1] 1 OK, 1 Please type ports separate with comma(,) and use hyphen(-) for range. [18100] 18100 OK, ['18100'] Define how many master processes will be created in the cluster per server.","title":"(3) Type Masters"},{"location":"run-and-deploy-fbctl/#4-type-information-of-slave","text":"How many replicas would you like to create on each master? [2] 2 OK, 2 Please type ports separate with comma(,) and use hyphen(-) for range. [18150-18151] 18150-18151 OK, ['18150-18151'] Define how many slave processes will be created for a master process.","title":"(4) Type information of slave"},{"location":"run-and-deploy-fbctl/#5-type-the-count-of-ssddisk-and-the-path-of-db-files","text":"How many sdd would you like to use? [3] 3 OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_","title":"(5) Type the count of SSD(disk) and the path of DB files"},{"location":"run-and-deploy-fbctl/#6-check-all-settings-finally","text":"Finally all settings will be shown and confirmation will be requested like below. +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y","title":"(6) Check all settings finally"},{"location":"run-and-deploy-fbctl/#7-deploy-cluster","text":"After deploying is completed, following messages are shown and fbctl of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLEAN | | nodeD | CLEAN | +-----------+--------+ OK Transfer install and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1 Cluster 1 selected. When an error occurs during deploying, error messages will be shown like below. Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If localhost(127.0.0.1) is not included in host information, this error occurs. Please add localhost in host list in this case. From now, user can start and manage clusters of LightningDB with Commands .","title":"(7) Deploy cluster"},{"location":"run-and-deploy-fbctl/#3-lightningdb-version-update","text":"In case of version update, 'deploy' command is used. > c 1 // alias of 'cluster use 1' > deploy (Watch out) Cluster 1 is already deployed. Do you want to deploy again? (y/n) [n] y","title":"3. LightningDB Version Update"},{"location":"run-and-deploy-fbctl/#1-select-installer_1","text":"Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin","title":"(1) Select installer"},{"location":"run-and-deploy-fbctl/#2-restore","text":"Do you want to restore conf? (y/n) y If the current settings will be reused, type 'y'.","title":"(2) Restore"},{"location":"run-and-deploy-fbctl/#3-check-all-settings-finally","text":"+-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | redis data path | ~/sata_ssd/ssd_ | | redis db path | ~/sata_ssd/ssd_ | | flash db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ Checking for cluster exist... +------+--------+ | HOST | STATUS | +------+--------+ Backup conf of cluster 1... OK, cluster_1_conf_bak_<time-stamp> Backup info of cluster 1 at nodeA... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeB... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeC... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeD... OK, cluster_1_bak_<time-stamp> Transfer installer and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1. Cluster 1 selected. Backup path of cluster: ${base-directory}/backup/cluster_${cluster-id}_bak_${time-stamp} Backup path of conf files: $FBAPTH/conf_backup/cluster_${cluster-id}_conf_bak_${time-stamp}","title":"(3) Check all settings finally"},{"location":"run-and-deploy-fbctl/#4-restart","text":"> cluster restart After restart, new version will be applied. If user types 'enter' without any text, the default value is applied. In some case, default value will not provided. \u21a9","title":"(4) Restart"},{"location":"try-with-zeppelin/","text":"1. Setting for Zeppelin \u00b6 You can try LightningDB in Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using fbctl before launching Zeppelin daemon. Secondly, in order to run LightingDB on the Spark, the jars in the LightingDB should be passed to the Spark. This can be done by adjusting SPARK_SUBMIT_OPTIONS in zeppllin-env.sh $ cp $ZEPPELIN_HOME/conf/zeppelin-env.sh.template $ZEPPELIN_HOME/conf/zeppelin-env.sh $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh # /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT is a path in which LightningDB is installed using fbctl. # This can be different if you installed LightningDB in different path. export SPARK_SUBMIT_OPTIONS=\"--jars $(find /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT/lib -name 'tsr2*' \\ -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' \\ -o -name 'gt-*' | tr '\\n' ',')\" Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start 2. Tutorial with Zeppelin \u00b6 After starting zeppelin daemon, you can access zeppelin UI using browser. The url is https://your-server-ip:8080 . There is a github page for tutorial . The repository includes a tool for generating sample data and a notebook for tutorial. You can import the tutorial notebook with its url. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of Spark driver is at least 10GB in Spark interpreter setting. Also, make sure that the timeout of shell command is at least 120000 ms.","title":"Try out with Zeppelin"},{"location":"try-with-zeppelin/#1-setting-for-zeppelin","text":"You can try LightningDB in Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using fbctl before launching Zeppelin daemon. Secondly, in order to run LightingDB on the Spark, the jars in the LightingDB should be passed to the Spark. This can be done by adjusting SPARK_SUBMIT_OPTIONS in zeppllin-env.sh $ cp $ZEPPELIN_HOME/conf/zeppelin-env.sh.template $ZEPPELIN_HOME/conf/zeppelin-env.sh $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh # /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT is a path in which LightningDB is installed using fbctl. # This can be different if you installed LightningDB in different path. export SPARK_SUBMIT_OPTIONS=\"--jars $(find /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT/lib -name 'tsr2*' \\ -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' \\ -o -name 'gt-*' | tr '\\n' ',')\" Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start","title":"1. Setting for Zeppelin"},{"location":"try-with-zeppelin/#2-tutorial-with-zeppelin","text":"After starting zeppelin daemon, you can access zeppelin UI using browser. The url is https://your-server-ip:8080 . There is a github page for tutorial . The repository includes a tool for generating sample data and a notebook for tutorial. You can import the tutorial notebook with its url. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of Spark driver is at least 10GB in Spark interpreter setting. Also, make sure that the timeout of shell command is at least 120000 ms.","title":"2. Tutorial with Zeppelin"}]}