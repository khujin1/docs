{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"1. LightningDB is \u00b6 A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data access based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data 2. Architecture \u00b6 Spark with Redis/Rocksdb key-value stores No I/O bottleneck due to redis in DRAM and RocksDB in SSDs due to the small-sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned 3. Features \u00b6 Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD lifetime","title":"Overview"},{"location":"#1-lightningdb-is","text":"A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data access based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data","title":"1. LightningDB is"},{"location":"#2-architecture","text":"Spark with Redis/Rocksdb key-value stores No I/O bottleneck due to redis in DRAM and RocksDB in SSDs due to the small-sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned","title":"2. Architecture"},{"location":"#3-features","text":"Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD lifetime","title":"3. Features"},{"location":"command-line-interface/","text":"Note Command Line Interface(CLI) of LightningDB supports not only deploy and start command but also many commands to access and manipulate data in LightningDB. 1. Cluster Commands \u00b6 If you want to see the list of cluster commands, use the cluster command without any option. ec2-user@flashbase:1> cluster NAME fbctl cluster - This is cluster command SYNOPSIS fbctl cluster COMMAND DESCRIPTION This is cluster command COMMANDS COMMAND is one of the following: add_slave Add slaves to cluster additionally clean Clean cluster configure create Create cluster ls Check cluster list rebalance Rebalance restart Restart redist cluster rowcount Query and show cluster row count start Start cluster stop Stop cluster use Change selected cluster (1) Cluster configure redis-{port}.conf is generated with using redis-{master/slave}.conf.template and redis.properties files. > cluster configure (2) Cluster start Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in ${SR2_HOME}/logs/redis/ 1 will be moved to ${SR2_HOME}/logs/redis/backup/ . Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in ${SR2_REDIS_DATA} Start redis-server process Start master and slave redis-server with ${SR2_HOME}/conf/redis/redis-{port}.conf file Log files will be saved in ${SR2_HOME}/logs/redis/ ec2-user@flashbase:1> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 Generate redis configuration files for master hosts sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up Errors ErrorCode 11 Redis-server(master) process with the same port is already running. To resolve this error, use cluster stop or kill {pid of the process} . $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. ErrorCode 12 Redis-server(slave) process with the same port is already running. To resolve this error, use cluster stop or kill {pid of the process} . $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. Conf file not exist Conf file is not found. To resove this error, use cluster configure and then cluster start . $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf Max try error \u200b For detail information, please check the log files. $ cluster start ... ClusterRedisError: Fail to start redis: max try exceed Recommendation Command: 'monitor' (3) Cluster create After checking the information of the cluster, create a cluster of LightningDB. Case 1) When redis-server processes are running, create a cluster only. ec2-user@flashbase:1>`cluster create` Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 Adding slots... - 127.0.0.1:18100, 3280 - 127.0.0.1:18103, 3276 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 Check cluster state and asign slot... Ok create cluster complete. Case 2) When redis-server processes are not running, create a cluster after launching redis-server processes with cluster start command. ec2-user@flashbase:4>`cluster create` Check status of hosts... OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 - 127.0.0.1:18100 Adding slots... - 127.0.0.1:18103, 3280 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 - 127.0.0.1:18100, 3276 Check cluster state and asign slot... Ok create cluster complete. Errors When redis servers are not running, this error(Errno 111) will occur. To solve this error, use cluster start command previously. ec2-user@flashbase:1>`cluster create` Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y 127.0.0.1:18100 - [Errno 111] Connection refused (4) Cluster stop \u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b ec2-user@flashbase:1> cluster stop Check status of hosts... OK Stopping master cluster of redis... cur: 5 / total: 5 cur: 0 / total: 5 Complete all redis process down Options Force to kill all redis-servers(master/slave) with SIGKILL --force (5) Cluster clean Remove conf files for redis-server and all data(aof, rdb, RocksDB) of LightningDB ec2-user@flashbase:1> cluster clean Removing redis generated master configuration files - 127.0.0.1 Removing flash db directory, appendonly and dump.rdb files in master - 127.0.0.1 Removing master node configuration - 127.0.0.1 (6) Cluster restart\u200b Process cluster stop and then cluster start .\u200b\u200b Options Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop Remove all data(aof, rdb, RocksDB, conf files) before starting again. --reset Process cluster create . This command should be called with --reset . --cluster (7) Cluster ls List the deployed clusters. ec2-user@flashbase:2> cluster ls [1, 2] (8) Cluster use Change the cluster to use FBCTL. Use cluster use or c commands. ec2-user@flashbase:2> cluster use 1 Cluster '1' selected. ec2-user@flashbase:1> c 2 Cluster '2' selected. (9) Cluster add_slave Warning Before using the add-slave command, ingestion to master nodes should be stopped. After replication and sync between master and slave are completed, ingestion will be available again. You can add a slave to a cluster that is configured only with the master without redundancy. Create cluster only with masters Procedure for configuring the test environment. If cluster with the only masters already exists, go to the add slave info . Proceed with the deploy. Enter 0 in replicas as shown below when deploy. ec2-user@flashbase:2> deploy 3 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.5a6a38.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] How many masters would you like to create on each host? [5] OK, 5 Please type ports separate with comma(,) and use hyphen(-) for range. [18300-18304] OK, ['18300-18304'] How many replicas would you like to create on each master? [0] OK, 0 How many ssd would you like to use? [3] OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18300-18304 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 3. Cluster '3' selected. When the deploy is complete, start and create the cluster. ec2-user@flashbase:3> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18300|18301|18302|18303|18304 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up ec2-user@flashbase:3> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18300 | MASTER | | 127.0.0.1 | 18301 | MASTER | | 127.0.0.1 | 18302 | MASTER | | 127.0.0.1 | 18303 | MASTER | | 127.0.0.1 | 18304 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18300 - 127.0.0.1:18303 - 127.0.0.1:18304 - 127.0.0.1:18301 - 127.0.0.1:18302 Adding slots... - 127.0.0.1:18300, 3280 - 127.0.0.1:18303, 3276 - 127.0.0.1:18304, 3276 - 127.0.0.1:18301, 3276 - 127.0.0.1:18302, 3276 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:3> Add slave info Open the conf file. ec2-user@flashbase:3> conf cluster You can modify redis.properties by entering the command as shown above. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) #export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) #export SR2_REDIS_SLAVE_PORTS=( $(seq 18600 18609) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Modify SR2_REDIS_SLAVE_HOSTS and SR2_REDIS_SLAVE_PORTS as shown below. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_SLAVE_PORTS=( $(seq 18350 18354) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Save the modification and exit. ec2-user@flashbase:3> conf cluster Check status of hosts... OK sync conf OK Complete edit Execute cluster add-slave command ec2-user@flashbase:3> cluster add-slave Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK clean redis conf, node conf, db data of master clean redis conf, node conf, db data of slave - 127.0.0.1 Backup redis slave log in each SLAVE hosts... - 127.0.0.1 create redis data directory in each SLAVE hosts - 127.0.0.1 sync conf OK Starting slave nodes : 127.0.0.1 : 18350|18351|18352|18353|18354 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up replicate [M] 127.0.0.1 18300 - [S] 127.0.0.1 18350 replicate [M] 127.0.0.1 18301 - [S] 127.0.0.1 18351 replicate [M] 127.0.0.1 18302 - [S] 127.0.0.1 18352 replicate [M] 127.0.0.1 18303 - [S] 127.0.0.1 18353 replicate [M] 127.0.0.1 18304 - [S] 127.0.0.1 18354 5 / 5 meet complete. Check configuration information ec2-user@flashbase:3> cli cluster nodes 0549ec03031213f95121ceff6c9c13800aef848c 127.0.0.1:18303 master - 0 1574132251126 3 connected 3280-6555 1b09519d37ebb1c09095158b4f1c9f318ddfc747 127.0.0.1:18352 slave a6a8013cf0032f0f36baec3162122b3d993dd2c8 0 1574132251025 6 connected c7dc4815e24054104dff61cac6b13256a84ac4ae 127.0.0.1:18353 slave 0549ec03031213f95121ceff6c9c13800aef848c 0 1574132251126 3 connected 0ab96cb79165ddca7d7134f80aea844bd49ae2e1 127.0.0.1:18351 slave 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 0 1574132250724 4 connected 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 127.0.0.1:18301 master - 0 1574132250524 4 connected 9832-13107 e67005a46984445e559a1408dd0a4b24a8c92259 127.0.0.1:18304 master - 0 1574132251126 5 connected 6556-9831 a6a8013cf0032f0f36baec3162122b3d993dd2c8 127.0.0.1:18302 master - 0 1574132251126 2 connected 13108-16383 492cdf4b1dedab5fb94e7129da2a0e05f6c46c4f 127.0.0.1:18350 slave 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 0 1574132251126 6 connected f9f7fcee9009f25618e63d2771ee2529f814c131 127.0.0.1:18354 slave e67005a46984445e559a1408dd0a4b24a8c92259 0 1574132250724 5 connected 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 127.0.0.1:18300 myself,master - 0 1574132250000 1 connected 0-3279 (10) Cluster rowcount Check the count of records that are stored in the cluster. ec2-user@flashbase:1> cluster rowcount 0 (11) Check the status of cluster With the following commands, you can check the status of the cluster. Send PING ec2-user@flashbase:1> cli ping --all alive redis 10/10 If a node does not reply, the fail node will be displayed like below. +-------+-----------------+--------+ | TYPE | ADDR | RESULT | +-------+-----------------+--------+ | Slave | 127.0.0.1:18352 | FAIL | +-------+-----------------+--------+ alive redis 9/10 Check the status of the cluster ec2-user@flashbase:1> cli cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:5 cluster_size:5 cluster_current_epoch:4 cluster_my_epoch:2 cluster_stats_messages_ping_sent:12 cluster_stats_messages_pong_sent:14 cluster_stats_messages_sent:26 cluster_stats_messages_ping_received:10 cluster_stats_messages_pong_received:12 cluster_stats_messages_meet_received:4 cluster_stats_messages_received:26 Check the list of the nodes those are organizing the cluster. ec2-user@flashbase:1> cli cluster nodes 559af5e90c3f2c92f19c927c29166c268d938e8f 127.0.0.1:18104 master - 0 1574127926000 4 connected 6556-9831 174e2a62722273fb83814c2f12e2769086c3d185 127.0.0.1:18101 myself,master - 0 1574127925000 3 connected 9832-13107 35ab4d3f7f487c5332d7943dbf4b20d5840053ea 127.0.0.1:18100 master - 0 1574127926000 1 connected 0-3279 f39ed05ace18e97f74c745636ea1d171ac1d456f 127.0.0.1:18103 master - 0 1574127927172 0 connected 3280-6555 9fd612b86a9ce1b647ba9170b8f4a8bfa5c875fc 127.0.0.1:18102 master - 0 1574127926171 2 connected 13108-16383 (12) Cluster tree User can check the status of master nodes and slaves and show which master and slave nodes are linked. ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(connected) |__ 127.0.0.1:18952(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) (13) Cluster failover If a master node is killed, its slave node will automatically promote after 'cluster-node-time' 2 . User can promote the slave node immediately by using the 'cluster failover' command. Step 1) Check the status of the cluster In this case, '127.0.0.1:18902' node is killed. ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(disconnected) <--- Killed! |__ 127.0.0.1:18952(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) Step 2) Do failover with 'cluster failover' command ec2-user@flashbase:9> cluster failover failover 127.0.0.1:18952 for 127.0.0.1:18902 OK ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(disconnected) <--- Killed! 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) 127.0.0.1:18952(connected) <--- Promoted to master! (14) Cluster failover With 'cluster failover' command, the killed node is restarted and added to the cluster as the slave node. ec2-user@flashbase:9> cluster failback run 127.0.0.1:18902 ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) 127.0.0.1:18952(connected) <--- Promoted to master! |__ 127.0.0.1:18902(connected) <--- Failbacked. Now this node is slave! 2. Thrift Server Commands \u00b6 If you want to see the list of Thrift Server commands, use the the thriftserver command without any option. NAME fbctl thriftserver SYNOPSIS fbctl thriftserver COMMAND COMMANDS COMMAND is one of the following: beeline Connect to thriftserver command line monitor Show thriftserver log restart Thriftserver restart start Start thriftserver stop Stop thriftserver (1) Thriftserver beeline Connect to the thrift server ec2-user@flashbase:1> thriftserver beeline Connecting... Connecting to jdbc:hive2://localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Supplied authorities: localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Resolved authority: localhost:13000 19/11/19 04:45:18 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:13000 Connected to: Spark SQL (version 2.3.1) Driver: Hive JDBC (version 1.2.1.spark2) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 1.2.1.spark2 by Apache Hive 0: jdbc:hive2://localhost:13000> show tables; +-----------+------------+--------------+--+ | database | tableName | isTemporary | +-----------+------------+--------------+--+ +-----------+------------+--------------+--+ No rows selected (0.55 seconds) Default value of db url to connect is jdbc:hive2://$HIVE_HOST:$HIVE_PORT You can modify $HIVE_HOST and $HIVE_PORT by the command conf thriftserver (2) Thriftserver monitor You can view the logs of the thrift server in real-time. ec2-user@flashbase:1> thriftserver monitor Press Ctrl-C for exit. 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-172-31-39-147.ap-northeast-2.compute.internal:35909 with 912.3 MB RAM, BlockManagerId(4, ip-172-31-39-147.ap-northeast-2.compute.internal, 35909, None) 19/11/19 04:43:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.39.147:53604) with ID 5 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ... (3) Thriftserver restart Restart the thrift server. ec2-user@flashbase:1> thriftserver restart no org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 to stop starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out (4) Start thriftserver Run the thrift server. ec2-user@flashbase:1> thriftserver start starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out You can view the logs through the command monitor . (5) Stop thriftserver Shut down the thrift server. ec2-user@flashbase:1> thriftserver stop stopping org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 (6) Conf thriftserver ec2-user@flashbase:1> conf thriftserver #!/bin/bash ############################################################################### # Common variables SPARK_CONF=${SPARK_CONF:-$SPARK_HOME/conf} SPARK_BIN=${SPARK_BIN:-$SPARK_HOME/bin} SPARK_SBIN=${SPARK_SBIN:-$SPARK_HOME/sbin} SPARK_LOG=${SPARK_LOG:-$SPARK_HOME/logs} SPARK_METRICS=${SPARK_CONF}/metrics.properties SPARK_UI_PORT=${SPARK_UI_PORT:-14050} EXECUTERS=12 EXECUTER_CORES=32 HIVE_METASTORE_URL='' HIVE_HOST=${HIVE_HOST:-localhost} HIVE_PORT=${HIVE_PORT:-13000} COMMON_CLASSPATH=$(find $SR2_LIB -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' \\ -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ':') ############################################################################### # Driver DRIVER_MEMORY=6g DRIVER_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Execute EXECUTOR_MEMORY=2g EXECUTOR_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Thrift Server logs EVENT_LOG_ENABLED=false EVENT_LOG_DIR=/nvdrive0/thriftserver-event-logs EVENT_LOG_ROLLING_DIR=/nvdrive0/thriftserver-event-logs-rolling EVENT_LOG_SAVE_MIN=60 EXTRACTED_EVENT_LOG_SAVE_DAY=5 SPARK_LOG_SAVE_MIN=2000 ############## If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9 'cluster-node-time' can be set with using 'config set' command. Its default time is 1200,000 msec. \u21a9","title":"Command Line"},{"location":"command-line-interface/#1-cluster-commands","text":"If you want to see the list of cluster commands, use the cluster command without any option. ec2-user@flashbase:1> cluster NAME fbctl cluster - This is cluster command SYNOPSIS fbctl cluster COMMAND DESCRIPTION This is cluster command COMMANDS COMMAND is one of the following: add_slave Add slaves to cluster additionally clean Clean cluster configure create Create cluster ls Check cluster list rebalance Rebalance restart Restart redist cluster rowcount Query and show cluster row count start Start cluster stop Stop cluster use Change selected cluster (1) Cluster configure redis-{port}.conf is generated with using redis-{master/slave}.conf.template and redis.properties files. > cluster configure (2) Cluster start Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in ${SR2_HOME}/logs/redis/ 1 will be moved to ${SR2_HOME}/logs/redis/backup/ . Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in ${SR2_REDIS_DATA} Start redis-server process Start master and slave redis-server with ${SR2_HOME}/conf/redis/redis-{port}.conf file Log files will be saved in ${SR2_HOME}/logs/redis/ ec2-user@flashbase:1> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 Generate redis configuration files for master hosts sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up Errors ErrorCode 11 Redis-server(master) process with the same port is already running. To resolve this error, use cluster stop or kill {pid of the process} . $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. ErrorCode 12 Redis-server(slave) process with the same port is already running. To resolve this error, use cluster stop or kill {pid of the process} . $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. Conf file not exist Conf file is not found. To resove this error, use cluster configure and then cluster start . $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf Max try error \u200b For detail information, please check the log files. $ cluster start ... ClusterRedisError: Fail to start redis: max try exceed Recommendation Command: 'monitor' (3) Cluster create After checking the information of the cluster, create a cluster of LightningDB. Case 1) When redis-server processes are running, create a cluster only. ec2-user@flashbase:1>`cluster create` Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 Adding slots... - 127.0.0.1:18100, 3280 - 127.0.0.1:18103, 3276 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 Check cluster state and asign slot... Ok create cluster complete. Case 2) When redis-server processes are not running, create a cluster after launching redis-server processes with cluster start command. ec2-user@flashbase:4>`cluster create` Check status of hosts... OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 - 127.0.0.1:18100 Adding slots... - 127.0.0.1:18103, 3280 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 - 127.0.0.1:18100, 3276 Check cluster state and asign slot... Ok create cluster complete. Errors When redis servers are not running, this error(Errno 111) will occur. To solve this error, use cluster start command previously. ec2-user@flashbase:1>`cluster create` Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y 127.0.0.1:18100 - [Errno 111] Connection refused (4) Cluster stop \u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b ec2-user@flashbase:1> cluster stop Check status of hosts... OK Stopping master cluster of redis... cur: 5 / total: 5 cur: 0 / total: 5 Complete all redis process down Options Force to kill all redis-servers(master/slave) with SIGKILL --force (5) Cluster clean Remove conf files for redis-server and all data(aof, rdb, RocksDB) of LightningDB ec2-user@flashbase:1> cluster clean Removing redis generated master configuration files - 127.0.0.1 Removing flash db directory, appendonly and dump.rdb files in master - 127.0.0.1 Removing master node configuration - 127.0.0.1 (6) Cluster restart\u200b Process cluster stop and then cluster start .\u200b\u200b Options Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop Remove all data(aof, rdb, RocksDB, conf files) before starting again. --reset Process cluster create . This command should be called with --reset . --cluster (7) Cluster ls List the deployed clusters. ec2-user@flashbase:2> cluster ls [1, 2] (8) Cluster use Change the cluster to use FBCTL. Use cluster use or c commands. ec2-user@flashbase:2> cluster use 1 Cluster '1' selected. ec2-user@flashbase:1> c 2 Cluster '2' selected. (9) Cluster add_slave Warning Before using the add-slave command, ingestion to master nodes should be stopped. After replication and sync between master and slave are completed, ingestion will be available again. You can add a slave to a cluster that is configured only with the master without redundancy. Create cluster only with masters Procedure for configuring the test environment. If cluster with the only masters already exists, go to the add slave info . Proceed with the deploy. Enter 0 in replicas as shown below when deploy. ec2-user@flashbase:2> deploy 3 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.5a6a38.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] How many masters would you like to create on each host? [5] OK, 5 Please type ports separate with comma(,) and use hyphen(-) for range. [18300-18304] OK, ['18300-18304'] How many replicas would you like to create on each master? [0] OK, 0 How many ssd would you like to use? [3] OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18300-18304 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 3. Cluster '3' selected. When the deploy is complete, start and create the cluster. ec2-user@flashbase:3> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18300|18301|18302|18303|18304 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up ec2-user@flashbase:3> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18300 | MASTER | | 127.0.0.1 | 18301 | MASTER | | 127.0.0.1 | 18302 | MASTER | | 127.0.0.1 | 18303 | MASTER | | 127.0.0.1 | 18304 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18300 - 127.0.0.1:18303 - 127.0.0.1:18304 - 127.0.0.1:18301 - 127.0.0.1:18302 Adding slots... - 127.0.0.1:18300, 3280 - 127.0.0.1:18303, 3276 - 127.0.0.1:18304, 3276 - 127.0.0.1:18301, 3276 - 127.0.0.1:18302, 3276 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:3> Add slave info Open the conf file. ec2-user@flashbase:3> conf cluster You can modify redis.properties by entering the command as shown above. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) #export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) #export SR2_REDIS_SLAVE_PORTS=( $(seq 18600 18609) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Modify SR2_REDIS_SLAVE_HOSTS and SR2_REDIS_SLAVE_PORTS as shown below. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_SLAVE_PORTS=( $(seq 18350 18354) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Save the modification and exit. ec2-user@flashbase:3> conf cluster Check status of hosts... OK sync conf OK Complete edit Execute cluster add-slave command ec2-user@flashbase:3> cluster add-slave Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK clean redis conf, node conf, db data of master clean redis conf, node conf, db data of slave - 127.0.0.1 Backup redis slave log in each SLAVE hosts... - 127.0.0.1 create redis data directory in each SLAVE hosts - 127.0.0.1 sync conf OK Starting slave nodes : 127.0.0.1 : 18350|18351|18352|18353|18354 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up replicate [M] 127.0.0.1 18300 - [S] 127.0.0.1 18350 replicate [M] 127.0.0.1 18301 - [S] 127.0.0.1 18351 replicate [M] 127.0.0.1 18302 - [S] 127.0.0.1 18352 replicate [M] 127.0.0.1 18303 - [S] 127.0.0.1 18353 replicate [M] 127.0.0.1 18304 - [S] 127.0.0.1 18354 5 / 5 meet complete. Check configuration information ec2-user@flashbase:3> cli cluster nodes 0549ec03031213f95121ceff6c9c13800aef848c 127.0.0.1:18303 master - 0 1574132251126 3 connected 3280-6555 1b09519d37ebb1c09095158b4f1c9f318ddfc747 127.0.0.1:18352 slave a6a8013cf0032f0f36baec3162122b3d993dd2c8 0 1574132251025 6 connected c7dc4815e24054104dff61cac6b13256a84ac4ae 127.0.0.1:18353 slave 0549ec03031213f95121ceff6c9c13800aef848c 0 1574132251126 3 connected 0ab96cb79165ddca7d7134f80aea844bd49ae2e1 127.0.0.1:18351 slave 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 0 1574132250724 4 connected 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 127.0.0.1:18301 master - 0 1574132250524 4 connected 9832-13107 e67005a46984445e559a1408dd0a4b24a8c92259 127.0.0.1:18304 master - 0 1574132251126 5 connected 6556-9831 a6a8013cf0032f0f36baec3162122b3d993dd2c8 127.0.0.1:18302 master - 0 1574132251126 2 connected 13108-16383 492cdf4b1dedab5fb94e7129da2a0e05f6c46c4f 127.0.0.1:18350 slave 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 0 1574132251126 6 connected f9f7fcee9009f25618e63d2771ee2529f814c131 127.0.0.1:18354 slave e67005a46984445e559a1408dd0a4b24a8c92259 0 1574132250724 5 connected 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 127.0.0.1:18300 myself,master - 0 1574132250000 1 connected 0-3279 (10) Cluster rowcount Check the count of records that are stored in the cluster. ec2-user@flashbase:1> cluster rowcount 0 (11) Check the status of cluster With the following commands, you can check the status of the cluster. Send PING ec2-user@flashbase:1> cli ping --all alive redis 10/10 If a node does not reply, the fail node will be displayed like below. +-------+-----------------+--------+ | TYPE | ADDR | RESULT | +-------+-----------------+--------+ | Slave | 127.0.0.1:18352 | FAIL | +-------+-----------------+--------+ alive redis 9/10 Check the status of the cluster ec2-user@flashbase:1> cli cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:5 cluster_size:5 cluster_current_epoch:4 cluster_my_epoch:2 cluster_stats_messages_ping_sent:12 cluster_stats_messages_pong_sent:14 cluster_stats_messages_sent:26 cluster_stats_messages_ping_received:10 cluster_stats_messages_pong_received:12 cluster_stats_messages_meet_received:4 cluster_stats_messages_received:26 Check the list of the nodes those are organizing the cluster. ec2-user@flashbase:1> cli cluster nodes 559af5e90c3f2c92f19c927c29166c268d938e8f 127.0.0.1:18104 master - 0 1574127926000 4 connected 6556-9831 174e2a62722273fb83814c2f12e2769086c3d185 127.0.0.1:18101 myself,master - 0 1574127925000 3 connected 9832-13107 35ab4d3f7f487c5332d7943dbf4b20d5840053ea 127.0.0.1:18100 master - 0 1574127926000 1 connected 0-3279 f39ed05ace18e97f74c745636ea1d171ac1d456f 127.0.0.1:18103 master - 0 1574127927172 0 connected 3280-6555 9fd612b86a9ce1b647ba9170b8f4a8bfa5c875fc 127.0.0.1:18102 master - 0 1574127926171 2 connected 13108-16383 (12) Cluster tree User can check the status of master nodes and slaves and show which master and slave nodes are linked. ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(connected) |__ 127.0.0.1:18952(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) (13) Cluster failover If a master node is killed, its slave node will automatically promote after 'cluster-node-time' 2 . User can promote the slave node immediately by using the 'cluster failover' command. Step 1) Check the status of the cluster In this case, '127.0.0.1:18902' node is killed. ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(disconnected) <--- Killed! |__ 127.0.0.1:18952(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) Step 2) Do failover with 'cluster failover' command ec2-user@flashbase:9> cluster failover failover 127.0.0.1:18952 for 127.0.0.1:18902 OK ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18902(disconnected) <--- Killed! 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) 127.0.0.1:18952(connected) <--- Promoted to master! (14) Cluster failover With 'cluster failover' command, the killed node is restarted and added to the cluster as the slave node. ec2-user@flashbase:9> cluster failback run 127.0.0.1:18902 ec2-user@flashbase:9> cluster tree 127.0.0.1:18900(connected) |__ 127.0.0.1:18950(connected) 127.0.0.1:18901(connected) |__ 127.0.0.1:18951(connected) 127.0.0.1:18903(connected) |__ 127.0.0.1:18953(connected) 127.0.0.1:18904(connected) |__ 127.0.0.1:18954(connected) 127.0.0.1:18905(connected) |__ 127.0.0.1:18955(connected) 127.0.0.1:18906(connected) |__ 127.0.0.1:18956(connected) 127.0.0.1:18952(connected) <--- Promoted to master! |__ 127.0.0.1:18902(connected) <--- Failbacked. Now this node is slave!","title":"1. Cluster Commands"},{"location":"command-line-interface/#2-thrift-server-commands","text":"If you want to see the list of Thrift Server commands, use the the thriftserver command without any option. NAME fbctl thriftserver SYNOPSIS fbctl thriftserver COMMAND COMMANDS COMMAND is one of the following: beeline Connect to thriftserver command line monitor Show thriftserver log restart Thriftserver restart start Start thriftserver stop Stop thriftserver (1) Thriftserver beeline Connect to the thrift server ec2-user@flashbase:1> thriftserver beeline Connecting... Connecting to jdbc:hive2://localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Supplied authorities: localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Resolved authority: localhost:13000 19/11/19 04:45:18 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:13000 Connected to: Spark SQL (version 2.3.1) Driver: Hive JDBC (version 1.2.1.spark2) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 1.2.1.spark2 by Apache Hive 0: jdbc:hive2://localhost:13000> show tables; +-----------+------------+--------------+--+ | database | tableName | isTemporary | +-----------+------------+--------------+--+ +-----------+------------+--------------+--+ No rows selected (0.55 seconds) Default value of db url to connect is jdbc:hive2://$HIVE_HOST:$HIVE_PORT You can modify $HIVE_HOST and $HIVE_PORT by the command conf thriftserver (2) Thriftserver monitor You can view the logs of the thrift server in real-time. ec2-user@flashbase:1> thriftserver monitor Press Ctrl-C for exit. 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-172-31-39-147.ap-northeast-2.compute.internal:35909 with 912.3 MB RAM, BlockManagerId(4, ip-172-31-39-147.ap-northeast-2.compute.internal, 35909, None) 19/11/19 04:43:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.39.147:53604) with ID 5 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ... (3) Thriftserver restart Restart the thrift server. ec2-user@flashbase:1> thriftserver restart no org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 to stop starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out (4) Start thriftserver Run the thrift server. ec2-user@flashbase:1> thriftserver start starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out You can view the logs through the command monitor . (5) Stop thriftserver Shut down the thrift server. ec2-user@flashbase:1> thriftserver stop stopping org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 (6) Conf thriftserver ec2-user@flashbase:1> conf thriftserver #!/bin/bash ############################################################################### # Common variables SPARK_CONF=${SPARK_CONF:-$SPARK_HOME/conf} SPARK_BIN=${SPARK_BIN:-$SPARK_HOME/bin} SPARK_SBIN=${SPARK_SBIN:-$SPARK_HOME/sbin} SPARK_LOG=${SPARK_LOG:-$SPARK_HOME/logs} SPARK_METRICS=${SPARK_CONF}/metrics.properties SPARK_UI_PORT=${SPARK_UI_PORT:-14050} EXECUTERS=12 EXECUTER_CORES=32 HIVE_METASTORE_URL='' HIVE_HOST=${HIVE_HOST:-localhost} HIVE_PORT=${HIVE_PORT:-13000} COMMON_CLASSPATH=$(find $SR2_LIB -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' \\ -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ':') ############################################################################### # Driver DRIVER_MEMORY=6g DRIVER_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Execute EXECUTOR_MEMORY=2g EXECUTOR_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Thrift Server logs EVENT_LOG_ENABLED=false EVENT_LOG_DIR=/nvdrive0/thriftserver-event-logs EVENT_LOG_ROLLING_DIR=/nvdrive0/thriftserver-event-logs-rolling EVENT_LOG_SAVE_MIN=60 EXTRACTED_EVENT_LOG_SAVE_DAY=5 SPARK_LOG_SAVE_MIN=2000 ############## If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9 'cluster-node-time' can be set with using 'config set' command. Its default time is 1200,000 msec. \u21a9","title":"2. Thrift Server Commands"},{"location":"data-ingestion-and-querying/","text":"1. Create a table \u00b6 You can create tables in the metastore using standard DDL. CREATE TABLE `pcell` ( `event_time` STRING, `m_10_under` DOUBLE, `m_10_19` DOUBLE, `m_20_29` DOUBLE, `m_30_39` DOUBLE, `m_40_49` DOUBLE, `m_50_59` DOUBLE, `m_60_over` DOUBLE, `longitude` DOUBLE, `lattitude` DOUBLE, `geohash` STRING) USING r2 OPTIONS ( `table` '100', `host` 'localhost', `port` '18100', `partitions` 'event_time geohash', `mode` 'nvkvs', `at_least_one_partition_enabled` 'no', `rowstore` 'true' ) There are various options used to describe storage properties. table : Positive Integer. The identification of the table. Redis identifies a table with this value. host/port : The host/port of representative Redis Node. Using this host and port, Spark builds a Redis cluster client that retrieves and inserts data to the Redis cluster. partitions : The partitions columns. The partition column values are used to distribute data in Redis cluster. That is, the partition column values are concatenated with a colon(:) and used as KEY of Redis which is the criteria distributing data. For more information, you can refer to Keys distribution model page in Redis. Tip Deciding a partition column properly is a crucial factor for performance because it is related to sharding data to multiple Redis nodes. It is important to try to distribute KEYs to 16384 slots of REDIS evenly and to try to map at least 200 rows for each KEY. mode : 'nvkvs' for this field at_least_one_partition_enabled : yes or no. If yes, the queries which do not have partition filter are not permitted. rowstore : true or false. If yes, all columns are merged and stored in RockDB as one column. It enhances ingesting performance. However, the query performance can be dropped because there is overhead for parsing columns in the Redis layer when retrieving data from RockDB. Tip The metastore of LightningDB only contains metadata/schema of tables. The actual data are stored in FlashBase which consists of Redis & RockDB (Abbreviation: r2), and the table information is stored in metastore. 2. Data Ingestion \u00b6 (1) Insert data with DataFrameWriter You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". // Create source DataFrame. val df = spark.sqlContext.read.format(\"csv\") .option(\"header\", \"false\") .option(\"inferSchema\", \"true\") .load(\"/nvme/data_01/csv/\") // \"pcell\" is a name of table which has R2 options. df.write.insertInto(\"pcell\") (2) Insert data with INSERT INTO SELECT query -- pcell : table with R2 option -- csv_table : table with csv option -- udf : UDF can be used to transform original data. INSERT INTO pcell SELECT *, udf(event_time) FROM csv_table 3. Querying \u00b6 You can query data with SparkSQL interfaces such as DataFrames and Spark ThriftServer. Please refer to Spark SQL guide page .","title":"Data Ingestion and querying"},{"location":"data-ingestion-and-querying/#1-create-a-table","text":"You can create tables in the metastore using standard DDL. CREATE TABLE `pcell` ( `event_time` STRING, `m_10_under` DOUBLE, `m_10_19` DOUBLE, `m_20_29` DOUBLE, `m_30_39` DOUBLE, `m_40_49` DOUBLE, `m_50_59` DOUBLE, `m_60_over` DOUBLE, `longitude` DOUBLE, `lattitude` DOUBLE, `geohash` STRING) USING r2 OPTIONS ( `table` '100', `host` 'localhost', `port` '18100', `partitions` 'event_time geohash', `mode` 'nvkvs', `at_least_one_partition_enabled` 'no', `rowstore` 'true' ) There are various options used to describe storage properties. table : Positive Integer. The identification of the table. Redis identifies a table with this value. host/port : The host/port of representative Redis Node. Using this host and port, Spark builds a Redis cluster client that retrieves and inserts data to the Redis cluster. partitions : The partitions columns. The partition column values are used to distribute data in Redis cluster. That is, the partition column values are concatenated with a colon(:) and used as KEY of Redis which is the criteria distributing data. For more information, you can refer to Keys distribution model page in Redis. Tip Deciding a partition column properly is a crucial factor for performance because it is related to sharding data to multiple Redis nodes. It is important to try to distribute KEYs to 16384 slots of REDIS evenly and to try to map at least 200 rows for each KEY. mode : 'nvkvs' for this field at_least_one_partition_enabled : yes or no. If yes, the queries which do not have partition filter are not permitted. rowstore : true or false. If yes, all columns are merged and stored in RockDB as one column. It enhances ingesting performance. However, the query performance can be dropped because there is overhead for parsing columns in the Redis layer when retrieving data from RockDB. Tip The metastore of LightningDB only contains metadata/schema of tables. The actual data are stored in FlashBase which consists of Redis & RockDB (Abbreviation: r2), and the table information is stored in metastore.","title":"1. Create a table"},{"location":"data-ingestion-and-querying/#2-data-ingestion","text":"(1) Insert data with DataFrameWriter You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". // Create source DataFrame. val df = spark.sqlContext.read.format(\"csv\") .option(\"header\", \"false\") .option(\"inferSchema\", \"true\") .load(\"/nvme/data_01/csv/\") // \"pcell\" is a name of table which has R2 options. df.write.insertInto(\"pcell\") (2) Insert data with INSERT INTO SELECT query -- pcell : table with R2 option -- csv_table : table with csv option -- udf : UDF can be used to transform original data. INSERT INTO pcell SELECT *, udf(event_time) FROM csv_table","title":"2. Data Ingestion"},{"location":"data-ingestion-and-querying/#3-querying","text":"You can query data with SparkSQL interfaces such as DataFrames and Spark ThriftServer. Please refer to Spark SQL guide page .","title":"3. Querying"},{"location":"get-started-with-scratch/","text":"Note This page guides how to start LightningDB on CentOS manually. In case of using AWS EC2 Instance , please use Installation 1. Optimizing System Parameters \u00b6 (1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages 2. Setup Prerequisites \u00b6 bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install Go to the website: https://software.intel.com/en-us/mkl/choose-download/macos Register and login Select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download Unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh Choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings Append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks. 3. Session configuration files \u00b6 Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster' 4. Install and Start LightningDB \u00b6 With FBCTL provided by LightningDB, users can deploy and use LightningDB. Install FBCTL with the following command. $ pip insatll fbctl After installation is completed, start FBCTL with Commands","title":"Manual Installation"},{"location":"get-started-with-scratch/#1-optimizing-system-parameters","text":"(1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages","title":"1. Optimizing System Parameters"},{"location":"get-started-with-scratch/#2-setup-prerequisites","text":"bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install Go to the website: https://software.intel.com/en-us/mkl/choose-download/macos Register and login Select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download Unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh Choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings Append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks.","title":"2. Setup Prerequisites"},{"location":"get-started-with-scratch/#3-session-configuration-files","text":"Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster'","title":"3. Session configuration files"},{"location":"get-started-with-scratch/#4-install-and-start-lightningdb","text":"With FBCTL provided by LightningDB, users can deploy and use LightningDB. Install FBCTL with the following command. $ pip insatll fbctl After installation is completed, start FBCTL with Commands","title":"4. Install and Start LightningDB"},{"location":"install-fbctl/","text":"1. How to run FBCTL \u00b6 If you try to use FBCTL for the first time after the EC2 instance was created, please update FBCTL like below. pip install fbctl --upgrade --user (1) Run To run FBCTL, ${FBPATH} should be set. If not, the following error messages will be shown. To start using FBCTL, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In the case of EC2 Instance, this path is set automatically. Run FBCTL by typing 'fbctl' $ fbctl When FBCTL starts for the first time, you need to confirm 'base_directory'. [~/tsr2] 1 is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', you can modify 'base_directory'. If you logs in FBCTL normally, FBCTL starts on the last visited cluster. In the case of the first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that you are in Centos and '>' means that you are in FBCTL. (2) Log messages Log messages of FBCTL will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceeding of size limit. 2. Deploy LightningDB \u00b6 Deploy is the procedure that LightningDB is installed with the specified cluster number. You could make LightningDB cluster with the following command. > deploy 1 After deploy command, you should type the following information that provides its last used value. Installer Host Number of masters Replicas Number of SSD(disk) The prefix of DB path (used for 'redis data', 'redis DB path' and 'flash DB path') Use the below option not to save the last used value. > deploy --history-save=False (1) Select installer Select installer [ INSTALLER LIST ] (1) [DOWNLOAD] flashbase.dev.master.dbcb9e.bin (2) [LOCAL] flashbase.dev.master.dbcb9e.bin (3) [LOCAL] flashbase.trial.master.dbcb9e-dirty.bin (4) [LOCAL] flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin Tip LOCAL means installer file under path '$FBPATH/releases/' on your local. DOWNLOAD refers to a file that can be downloaded and up to 5 files are displayed in the latest order. To confirm the recommended FlashBase version, use Release Notes Select a number to use that file. Type DOWNLOAD will be used after downloading. The downloaded file is saved in path '$FBPATH/releases'. Select installer [ INSTALLER LIST ] (empty) Please enter file path or url of the installer you want to use you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/latest/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin If the installer list is empty like above, you can also use file path or URL. If you enter URL, download the file and use it. The downloaded file is saved in path '$FBPATH/releases'. (2) Type Hosts IP address or hostname can be used. In the case of several hosts, the list can be separated by comma(','). Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] (3) Type Masters How many masters would you like to create on each host? [10] OK, 10 Please type ports separate with a comma(,) and use a hyphen(-) for range. [18100-18109] OK, ['18100-18109'] Define how many master processes will be created in the cluster per server. Tip To create a cluster, 3 master processes should be included at least. (4) Type information of slave How many replicas would you like to create on each master? [0] OK, 0 Define how many slave processes will be created for a master process. (5) Type the count of SSD(disk) and the path of DB files How many ssd would you like to use? [4] OK, 4 Type prefix of db path [/nvme/data_] OK, /nvme/data_ (6) Check all settings finally Finally, all settings will be shown and confirmation will be requested like below. +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18100-18109 | | ssd count | 4 | | db path | /nvme/data_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y (7) Deploy cluster After deploying is completed, the following messages are shown and FBCTL of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 1. Cluster '1' selected. When an error occurs during deploying, error messages will be shown like below. (8) Errors Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of the host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If the localhost(127.0.0.1) is not included in host information, this error occurs. Please add the localhost in the host list in this case. 3. Start LightningDB \u00b6 Create a cluster of LightningDB using 'cluster create' command. ec2-user@flashbase:1> cluster create Check status of hosts... OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104|18105|18106|18107|18108|18109 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | | 127.0.0.1 | 18105 | MASTER | | 127.0.0.1 | 18106 | MASTER | | 127.0.0.1 | 18107 | MASTER | | 127.0.0.1 | 18108 | MASTER | | 127.0.0.1 | 18109 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18107 - 127.0.0.1:18106 - 127.0.0.1:18101 - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18109 - 127.0.0.1:18102 - 127.0.0.1:18108 - 127.0.0.1:18105 - 127.0.0.1:18104 Adding slots... - 127.0.0.1:18107, 1642 - 127.0.0.1:18106, 1638 - 127.0.0.1:18101, 1638 - 127.0.0.1:18100, 1638 - 127.0.0.1:18103, 1638 - 127.0.0.1:18109, 1638 - 127.0.0.1:18102, 1638 - 127.0.0.1:18108, 1638 - 127.0.0.1:18105, 1638 - 127.0.0.1:18104, 1638 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:1> cli ping --all alive redis 10/10 ec2-user@flashbase:1> From now, you can try ingestion and query in LightningDB with Zeppelin . And for further information about commands of FBCTL, please use Command Line . If you type 'enter' without any text, the default value is applied. In some cases, the default value will not be provided. \u21a9","title":"Installation"},{"location":"install-fbctl/#1-how-to-run-fbctl","text":"If you try to use FBCTL for the first time after the EC2 instance was created, please update FBCTL like below. pip install fbctl --upgrade --user (1) Run To run FBCTL, ${FBPATH} should be set. If not, the following error messages will be shown. To start using FBCTL, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In the case of EC2 Instance, this path is set automatically. Run FBCTL by typing 'fbctl' $ fbctl When FBCTL starts for the first time, you need to confirm 'base_directory'. [~/tsr2] 1 is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', you can modify 'base_directory'. If you logs in FBCTL normally, FBCTL starts on the last visited cluster. In the case of the first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that you are in Centos and '>' means that you are in FBCTL. (2) Log messages Log messages of FBCTL will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceeding of size limit.","title":"1. How to run FBCTL"},{"location":"install-fbctl/#2-deploy-lightningdb","text":"Deploy is the procedure that LightningDB is installed with the specified cluster number. You could make LightningDB cluster with the following command. > deploy 1 After deploy command, you should type the following information that provides its last used value. Installer Host Number of masters Replicas Number of SSD(disk) The prefix of DB path (used for 'redis data', 'redis DB path' and 'flash DB path') Use the below option not to save the last used value. > deploy --history-save=False (1) Select installer Select installer [ INSTALLER LIST ] (1) [DOWNLOAD] flashbase.dev.master.dbcb9e.bin (2) [LOCAL] flashbase.dev.master.dbcb9e.bin (3) [LOCAL] flashbase.trial.master.dbcb9e-dirty.bin (4) [LOCAL] flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin Tip LOCAL means installer file under path '$FBPATH/releases/' on your local. DOWNLOAD refers to a file that can be downloaded and up to 5 files are displayed in the latest order. To confirm the recommended FlashBase version, use Release Notes Select a number to use that file. Type DOWNLOAD will be used after downloading. The downloaded file is saved in path '$FBPATH/releases'. Select installer [ INSTALLER LIST ] (empty) Please enter file path or url of the installer you want to use you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/latest/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin If the installer list is empty like above, you can also use file path or URL. If you enter URL, download the file and use it. The downloaded file is saved in path '$FBPATH/releases'. (2) Type Hosts IP address or hostname can be used. In the case of several hosts, the list can be separated by comma(','). Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] (3) Type Masters How many masters would you like to create on each host? [10] OK, 10 Please type ports separate with a comma(,) and use a hyphen(-) for range. [18100-18109] OK, ['18100-18109'] Define how many master processes will be created in the cluster per server. Tip To create a cluster, 3 master processes should be included at least. (4) Type information of slave How many replicas would you like to create on each master? [0] OK, 0 Define how many slave processes will be created for a master process. (5) Type the count of SSD(disk) and the path of DB files How many ssd would you like to use? [4] OK, 4 Type prefix of db path [/nvme/data_] OK, /nvme/data_ (6) Check all settings finally Finally, all settings will be shown and confirmation will be requested like below. +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18100-18109 | | ssd count | 4 | | db path | /nvme/data_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y (7) Deploy cluster After deploying is completed, the following messages are shown and FBCTL of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 1. Cluster '1' selected. When an error occurs during deploying, error messages will be shown like below. (8) Errors Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of the host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If the localhost(127.0.0.1) is not included in host information, this error occurs. Please add the localhost in the host list in this case.","title":"2. Deploy LightningDB"},{"location":"install-fbctl/#3-start-lightningdb","text":"Create a cluster of LightningDB using 'cluster create' command. ec2-user@flashbase:1> cluster create Check status of hosts... OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104|18105|18106|18107|18108|18109 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | | 127.0.0.1 | 18105 | MASTER | | 127.0.0.1 | 18106 | MASTER | | 127.0.0.1 | 18107 | MASTER | | 127.0.0.1 | 18108 | MASTER | | 127.0.0.1 | 18109 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18107 - 127.0.0.1:18106 - 127.0.0.1:18101 - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18109 - 127.0.0.1:18102 - 127.0.0.1:18108 - 127.0.0.1:18105 - 127.0.0.1:18104 Adding slots... - 127.0.0.1:18107, 1642 - 127.0.0.1:18106, 1638 - 127.0.0.1:18101, 1638 - 127.0.0.1:18100, 1638 - 127.0.0.1:18103, 1638 - 127.0.0.1:18109, 1638 - 127.0.0.1:18102, 1638 - 127.0.0.1:18108, 1638 - 127.0.0.1:18105, 1638 - 127.0.0.1:18104, 1638 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:1> cli ping --all alive redis 10/10 ec2-user@flashbase:1> From now, you can try ingestion and query in LightningDB with Zeppelin . And for further information about commands of FBCTL, please use Command Line . If you type 'enter' without any text, the default value is applied. In some cases, the default value will not be provided. \u21a9","title":"3. Start LightningDB"},{"location":"prerequisite/","text":"Note This page guides how to start LightningDB automatically only for the case of AWS EC2 Instance . 1. Create EC2 Instance \u00b6 Amazon Machine Image(AMI) for LightningDB can be found in 'AWS Marketplace' and user can create EC2 Instance with the AMI. To use LightningDB in a new Instance, the size of the root volume should be 15GiB at least. To use Web UI of HDFS, YARN, Spark and Zeppelin, you should add the following ports to 'Edit inbound rules' of 'Security groups' in EC2 Instance. Service Port HDFS 50070 YARN 8088 Spark 4040 Zeppelin 8080 2. Access EC2 Instance \u00b6 Create a EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS} 3. Setup environment \u00b6 When you access EC2 Instance, the following jobs are already done. Create and exchange SSH KEY for user authentication Mount disks Warning Before starting LightningDB, please check if the disk mount is completed using 'lsblk' like below. [ec2-user@ip-172-31-34-115 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 10G 0 disk \u2514\u2500xvda1 202:1 0 10G 0 part / nvme0n1 259:0 0 1.7T 0 disk /nvme/data_01 nvme1n1 259:1 0 1.7T 0 disk /nvme/data_02 nvme3n1 259:2 0 1.7T 0 disk /nvme/data_03 nvme2n1 259:3 0 1.7T 0 disk /nvme/data_04 Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration Tip To launch Spark application on YARN, start YARN with running 'start-dfs.sh' and 'start-yarn.sh' in order. 4. Start LightningDB \u00b6 LightningDB provides FBCTL that is introduced in Installation . With FBCTL, you can deploy and use LightningDB. LightningDB supports Zeppelin to provide the convenience of ingestion and querying data of LightningDB. About Zeppelin, Try out with Zeppelin page provides some guides.","title":"Prerequisite"},{"location":"prerequisite/#1-create-ec2-instance","text":"Amazon Machine Image(AMI) for LightningDB can be found in 'AWS Marketplace' and user can create EC2 Instance with the AMI. To use LightningDB in a new Instance, the size of the root volume should be 15GiB at least. To use Web UI of HDFS, YARN, Spark and Zeppelin, you should add the following ports to 'Edit inbound rules' of 'Security groups' in EC2 Instance. Service Port HDFS 50070 YARN 8088 Spark 4040 Zeppelin 8080","title":"1. Create EC2 Instance"},{"location":"prerequisite/#2-access-ec2-instance","text":"Create a EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS}","title":"2. Access EC2 Instance"},{"location":"prerequisite/#3-setup-environment","text":"When you access EC2 Instance, the following jobs are already done. Create and exchange SSH KEY for user authentication Mount disks Warning Before starting LightningDB, please check if the disk mount is completed using 'lsblk' like below. [ec2-user@ip-172-31-34-115 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 10G 0 disk \u2514\u2500xvda1 202:1 0 10G 0 part / nvme0n1 259:0 0 1.7T 0 disk /nvme/data_01 nvme1n1 259:1 0 1.7T 0 disk /nvme/data_02 nvme3n1 259:2 0 1.7T 0 disk /nvme/data_03 nvme2n1 259:3 0 1.7T 0 disk /nvme/data_04 Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration Tip To launch Spark application on YARN, start YARN with running 'start-dfs.sh' and 'start-yarn.sh' in order.","title":"3. Setup environment"},{"location":"prerequisite/#4-start-lightningdb","text":"LightningDB provides FBCTL that is introduced in Installation . With FBCTL, you can deploy and use LightningDB. LightningDB supports Zeppelin to provide the convenience of ingestion and querying data of LightningDB. About Zeppelin, Try out with Zeppelin page provides some guides.","title":"4. Start LightningDB"},{"location":"release-note/","text":"1. Recommended Versions \u00b6 LightningDB ver 1.0 1 2. Release Notes \u00b6 Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions Copy link address with Right-Clicking and paste when you try to deploy LightningDB in FBCTL. \u21a9","title":"Release Notes"},{"location":"release-note/#1-recommended-versions","text":"LightningDB ver 1.0 1","title":"1. Recommended Versions"},{"location":"release-note/#2-release-notes","text":"Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions Copy link address with Right-Clicking and paste when you try to deploy LightningDB in FBCTL. \u21a9","title":"2. Release Notes"},{"location":"try-with-zeppelin/","text":"1. Setting for Zeppelin \u00b6 You can try LightningDB in the Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using Installation before launching the Zeppelin daemon. Secondly, to run LightningDB on the Spark, the jars in the LightningDB should be passed to the Spark. When EC2 Instance is initialized, the environment variable ( $SPARK_SUBMIT_OPTIONS ) is configured for this reason. Thus just need to check the setting in zeppelin-env.sh . $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh ... LIGHTNINGDB_LIB_PATH=$(eval echo $(cat $FBPATH/config | head -n 1 | awk {'print $2'}))/cluster_$(cat $FBPATH/HEAD)/tsr2-assembly-1.0.0-SNAPSHOT/lib/ if [[ -e $LIGHTNINGDB_LIB_PATH ]]; then export SPARK_SUBMIT_OPTIONS=\"--jars $(find $LIGHTNINGDB_LIB_PATH -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ',')\" fi ... Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start 2. Tutorial with Zeppelin \u00b6 After starting zeppelin daemon, you can access zeppelin UI using a browser. The URL is https://your-server-ip:8080 . Tip We recommend that you proceed with the tutorial at the Chrome browser. There is a github page for tutorial . The repository includes a tool for generating sample csv data and a notebook for the tutorial. You can import the tutorial notebook with its URL. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of the Spark driver is at least 10GB in the Spark interpreter setting. Also, make sure that the timeout of a shell command is at least 120000 ms.","title":"Try out with Zeppelin"},{"location":"try-with-zeppelin/#1-setting-for-zeppelin","text":"You can try LightningDB in the Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using Installation before launching the Zeppelin daemon. Secondly, to run LightningDB on the Spark, the jars in the LightningDB should be passed to the Spark. When EC2 Instance is initialized, the environment variable ( $SPARK_SUBMIT_OPTIONS ) is configured for this reason. Thus just need to check the setting in zeppelin-env.sh . $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh ... LIGHTNINGDB_LIB_PATH=$(eval echo $(cat $FBPATH/config | head -n 1 | awk {'print $2'}))/cluster_$(cat $FBPATH/HEAD)/tsr2-assembly-1.0.0-SNAPSHOT/lib/ if [[ -e $LIGHTNINGDB_LIB_PATH ]]; then export SPARK_SUBMIT_OPTIONS=\"--jars $(find $LIGHTNINGDB_LIB_PATH -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ',')\" fi ... Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start","title":"1. Setting for Zeppelin"},{"location":"try-with-zeppelin/#2-tutorial-with-zeppelin","text":"After starting zeppelin daemon, you can access zeppelin UI using a browser. The URL is https://your-server-ip:8080 . Tip We recommend that you proceed with the tutorial at the Chrome browser. There is a github page for tutorial . The repository includes a tool for generating sample csv data and a notebook for the tutorial. You can import the tutorial notebook with its URL. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of the Spark driver is at least 10GB in the Spark interpreter setting. Also, make sure that the timeout of a shell command is at least 120000 ms.","title":"2. Tutorial with Zeppelin"},{"location":"version-update/","text":"You can update LightningDB by using the 'deploy' command. > c 1 // alias of 'cluster use 1' > deploy (Watch out) Cluster 1 is already deployed. Do you want to deploy again? (y/n) [n] y (1) Select installer Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or URL of the installer you want to use. you can also add a file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin (2) Restore Do you want to restore conf? (y/n) y If the current settings will be reused, type 'y'. (3) Check all settings finally +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | redis data path | ~/sata_ssd/ssd_ | | redis db path | ~/sata_ssd/ssd_ | | flash db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ Checking for cluster exist... +------+--------+ | HOST | STATUS | +------+--------+ Backup conf of cluster 1... OK, cluster_1_conf_bak_<time-stamp> Backup info of cluster 1 at nodeA... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeB... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeC... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeD... OK, cluster_1_bak_<time-stamp> Transfer installer and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1. Cluster 1 selected. Backup path of cluster: ${base-directory}/backup/cluster_${cluster-id}_bak_${time-stamp} Backup path of conf files: $FBAPTH/conf_backup/cluster_${cluster-id}_conf_bak_${time-stamp} (4) Restart > cluster restart After the restart, the new version will be applied.","title":"Version update"}]}