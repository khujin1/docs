{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"1. LightningDB is \u00b6 A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data accesses based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data 2. Architecture \u00b6 Spark with Redis/Rocksdb key value stores No I/O bottleneck due to redis in DRAM and rocksdb in SSDs due to the small sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned 3. Features \u00b6 Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering \u2022High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD life time","title":"Introduction"},{"location":"#1-lightningdb-is","text":"A distributed in-memory DBMS for real-time big data analytics Realtime ingestion and analytics for large scale data Advantages in random small data accesses based on DRAM/SSD resident KV Store Optimized for time series data and geospatial data","title":"1. LightningDB is"},{"location":"#2-architecture","text":"Spark with Redis/Rocksdb key value stores No I/O bottleneck due to redis in DRAM and rocksdb in SSDs due to the small sized key/value I/O and DRAM/SSDs\u2019 short latency (~200us) Filter predicates push down to redis and only associated partitions are chosen to be scanned","title":"2. Architecture"},{"location":"#3-features","text":"Ingestion performance (500,000 records/sec/node) Extreme partitioning (up-to 2 billion partitions for a single node) Real-time query performance by using fine-grained partitions and filter acceleration (vector processing by exploiting XEON SIMD instructions) Column-store / row-store support DRAM - SSD - HDD Tiering \u2022High compression ratio and compression speed (Gzip level compression ratio w/ LZ4 level speed) Low Write Amplification for SSD life time","title":"3. Features"},{"location":"FAQ/","text":"Q1: LightningDB is free? A1: Yes, free license. Q2: Is there any presentation experience in major conference? A2: Yes, in Spark AI Summit Europe 2019 Apache Spark AI Use Case in Telco: Network Quality Analysis and Prediction","title":"FAQ"},{"location":"Support/","text":"Commercial Support \u00b6 ... Trouble-shooting \u00b6 ...","title":"Commercial Support"},{"location":"Support/#commercial-support","text":"...","title":"Commercial Support"},{"location":"Support/#trouble-shooting","text":"...","title":"Trouble-shooting"},{"location":"command-line-interface/","text":"1. Cluster Commands \u00b6 If you want to see the list of cluster commands, use the 'cluster' command without any option. ec2-user@flashbase:1> cluster NAME fbctl cluster - This is cluster command SYNOPSIS fbctl cluster COMMAND DESCRIPTION This is cluster command COMMANDS COMMAND is one of the following: add_slave Add slaves to cluster additionally clean Clean cluster configure create Create cluster ls Check cluster list rebalance Rebalance restart Restart redist cluster rowcount Query and show cluster row count start Start cluster stop Stop cluster use Change selected cluster (1) cluster configure \u00b6 redis-{port}.conf is generated with using redis-{master/slave}.conf.template and redis.properties. > cluster configure (2) cluster start \u00b6 Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in '${SR2_HOME} 1 /logs/redis/' will be moved to '${SR2_HOME}/logs/redis/backup/'. Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in '${SR2_REDIS_DATA}' Start redis-server process Start master and slave redis-server with '${SR2_HOME}/conf/redis/redis-{port}.conf' file Log files will be saved in '${SR2_HOME}/logs/redis/' ec2-user@flashbase:1> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 Generate redis configuration files for master hosts sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up Errors ErrorCode 11 Redis-server(master) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. ErrorCode 12 Redis-server(slave) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. Conf file not exist Conf file is not found. To resove this error, use 'cluster configure' and then 'cluster start'. cluster configure \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\uc2dc\ud0a8 \ud6c4 cluster start\ub97c \uc9c4\ud589\ud558\uc138\uc694. $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf max try error \u200b For detail information, please check log files. $ cluster start ... max try error (3) cluster create \u00b6 After checking information of the cluster, create cluster of LightningDB. ec2-user@flashbase:1> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 Adding slots... - 127.0.0.1:18100, 3280 - 127.0.0.1:18103, 3276 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 Check cluster state and asign slot... Ok create cluster complete. Errors When redis servers are not running, this error(Errno 111) will occur. To solve this error, use 'cluster start' command previously. ec2-user@flashbase:1> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y 127.0.0.1:18100 - [Errno 111] Connection refused \u200b\u200b (4) cluster stop \u00b6 \u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b ec2-user@flashbase:1> cluster stop Check status of hosts... OK Stopping master cluster of redis... cur: 5 / total: 5 cur: 0 / total: 5 Complete all redis process down Options Force to kill all redis-servers(master/slave) with SIGKILL --force (5) cluster clean \u00b6 Remove conf files for redis-server and all data(aof, rdb, RocksDB) of LightningDB ec2-user@flashbase:1> cluster clean Removing redis generated master configuration files - 127.0.0.1 Removing flash db directory, appendonly and dump.rdb files in master - 127.0.0.1 Removing master node configuration - 127.0.0.1 (6) cluster restart\u200b \u00b6 Process 'cluster stop' and then 'cluster start'.\u200b\u200b Options Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop Remove all data(aof, rdb, RocksDB, conf files) before start again. --reset Process 'cluster create'. This command should be called with '--reset'. --cluster (7) cluster ls \u00b6 List up the deployed clusters. ec2-user@flashbase:2> cluster ls [1, 2] (8) cluster use \u00b6 Change the cluster to use FBCTL. Use 'cluster use' or 'c' commands. ec2-user@flashbase:2> cluster use 1 Cluster '1' selected. ec2-user@flashbase:1> c 2 Cluster '2' selected. (9) cluster add_slave \u00b6 You can add a slave to a cluster that is configured only with master without redundancy. create cluster only with master Procedure for configuring the test environment. If cluster with only master already exists, go to the add slave info . Proceed with the deploy. Enter 0 in replicas as shown below when deploy. ec2-user@flashbase:2> deploy 3 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.5a6a38.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] How many masters would you like to create on each host? [5] OK, 5 Please type ports separate with comma(,) and use hyphen(-) for range. [18300-18304] OK, ['18300-18304'] How many replicas would you like to create on each master? [0] OK, 0 How many ssd would you like to use? [3] OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18300-18304 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 3. Cluster '3' selected. When the deploy is complete, start and create the cluster. ec2-user@flashbase:3> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18300|18301|18302|18303|18304 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up ec2-user@flashbase:3> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18300 | MASTER | | 127.0.0.1 | 18301 | MASTER | | 127.0.0.1 | 18302 | MASTER | | 127.0.0.1 | 18303 | MASTER | | 127.0.0.1 | 18304 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18300 - 127.0.0.1:18303 - 127.0.0.1:18304 - 127.0.0.1:18301 - 127.0.0.1:18302 Adding slots... - 127.0.0.1:18300, 3280 - 127.0.0.1:18303, 3276 - 127.0.0.1:18304, 3276 - 127.0.0.1:18301, 3276 - 127.0.0.1:18302, 3276 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:3> add slave info Open conf file. ec2-user@flashbase:3> conf cluster You can modify redis.properties by entering the command as shown above. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) #export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) #export SR2_REDIS_SLAVE_PORTS=( $(seq 18600 18609) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Modify SR2_REDIS_SLAVE_HOSTS and SR2_REDIS_SLAVE_PORTS as shown below. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_SLAVE_PORTS=( $(seq 18350 18354) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Save the modification and exit. ec2-user@flashbase:3> conf cluster Check status of hosts... OK sync conf OK Complete edit execute command add-slave ec2-user@flashbase:3> cluster add-slave Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK clean redis conf, node conf, db data of master clean redis conf, node conf, db data of slave - 127.0.0.1 Backup redis slave log in each SLAVE hosts... - 127.0.0.1 create redis data directory in each SLAVE hosts - 127.0.0.1 sync conf OK Starting slave nodes : 127.0.0.1 : 18350|18351|18352|18353|18354 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up replicate [M] 127.0.0.1 18300 - [S] 127.0.0.1 18350 replicate [M] 127.0.0.1 18301 - [S] 127.0.0.1 18351 replicate [M] 127.0.0.1 18302 - [S] 127.0.0.1 18352 replicate [M] 127.0.0.1 18303 - [S] 127.0.0.1 18353 replicate [M] 127.0.0.1 18304 - [S] 127.0.0.1 18354 1 / 5 meet complete. 2 / 5 meet complete. 3 / 5 meet complete. 4 / 5 meet complete. 5 / 5 meet complete. check configuration information ec2-user@flashbase:3> cli cluster nodes 0549ec03031213f95121ceff6c9c13800aef848c 127.0.0.1:18303 master - 0 1574132251126 3 connected 3280-6555 1b09519d37ebb1c09095158b4f1c9f318ddfc747 127.0.0.1:18352 slave a6a8013cf0032f0f36baec3162122b3d993dd2c8 0 1574132251025 6 connected c7dc4815e24054104dff61cac6b13256a84ac4ae 127.0.0.1:18353 slave 0549ec03031213f95121ceff6c9c13800aef848c 0 1574132251126 3 connected 0ab96cb79165ddca7d7134f80aea844bd49ae2e1 127.0.0.1:18351 slave 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 0 1574132250724 4 connected 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 127.0.0.1:18301 master - 0 1574132250524 4 connected 9832-13107 e67005a46984445e559a1408dd0a4b24a8c92259 127.0.0.1:18304 master - 0 1574132251126 5 connected 6556-9831 a6a8013cf0032f0f36baec3162122b3d993dd2c8 127.0.0.1:18302 master - 0 1574132251126 2 connected 13108-16383 492cdf4b1dedab5fb94e7129da2a0e05f6c46c4f 127.0.0.1:18350 slave 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 0 1574132251126 6 connected f9f7fcee9009f25618e63d2771ee2529f814c131 127.0.0.1:18354 slave e67005a46984445e559a1408dd0a4b24a8c92259 0 1574132250724 5 connected 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 127.0.0.1:18300 myself,master - 0 1574132250000 1 connected 0-3279 (10) cluster rowcount \u00b6 Check the count of records that are stored in the cluster. ec2-user@flashbase:1> cluster rowcount 0 (11) Check status of cluster \u00b6 With the following commands, you can check the status of the cluster. Send PING > cli ping --all Check the status of the cluster ec2-user@flashbase:1> cli cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:5 cluster_size:5 cluster_current_epoch:4 cluster_my_epoch:2 cluster_stats_messages_ping_sent:12 cluster_stats_messages_pong_sent:14 cluster_stats_messages_sent:26 cluster_stats_messages_ping_received:10 cluster_stats_messages_pong_received:12 cluster_stats_messages_meet_received:4 cluster_stats_messages_received:26 Check the list of the nodes those are organizing the cluster. ec2-user@flashbase:1> cli cluster nodes 559af5e90c3f2c92f19c927c29166c268d938e8f 127.0.0.1:18104 master - 0 1574127926000 4 connected 6556-9831 174e2a62722273fb83814c2f12e2769086c3d185 127.0.0.1:18101 myself,master - 0 1574127925000 3 connected 9832-13107 35ab4d3f7f487c5332d7943dbf4b20d5840053ea 127.0.0.1:18100 master - 0 1574127926000 1 connected 0-3279 f39ed05ace18e97f74c745636ea1d171ac1d456f 127.0.0.1:18103 master - 0 1574127927172 0 connected 3280-6555 9fd612b86a9ce1b647ba9170b8f4a8bfa5c875fc 127.0.0.1:18102 master - 0 1574127926171 2 connected 13108-16383 2. Thrift Server Commands \u00b6 If you want to see the list of Thrift Server commands, use the 'thriftserver' command without any option. NAME fbctl thriftserver SYNOPSIS fbctl thriftserver COMMAND COMMANDS COMMAND is one of the following: beeline Connect to thriftserver command line monitor Show thriftserver log restart Thriftserver restart start Start thriftserver stop Stop thriftserver (1) thriftserver beeline \u00b6 Connect to the thrift server ec2-user@flashbase:1> thriftserver beeline Connecting... Connecting to jdbc:hive2://localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Supplied authorities: localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Resolved authority: localhost:13000 19/11/19 04:45:18 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:13000 Connected to: Spark SQL (version 2.3.1) Driver: Hive JDBC (version 1.2.1.spark2) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 1.2.1.spark2 by Apache Hive 0: jdbc:hive2://localhost:13000> show tables; +-----------+------------+--------------+--+ | database | tableName | isTemporary | +-----------+------------+--------------+--+ +-----------+------------+--------------+--+ No rows selected (0.55 seconds) Default value of db url to connect is jdbc:hive2://$HIVE_HOST:$HIVE_PORT You can modify $HIVE_HOST and $HIVE_PORT by command conf ths (2) thriftserver monitor \u00b6 You can view the logs of the thrift server in real time. ec2-user@flashbase:1> thriftserver monitor Press Ctrl-C for exit. 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-172-31-39-147.ap-northeast-2.compute.internal:35909 with 912.3 MB RAM, BlockManagerId(4, ip-172-31-39-147.ap-northeast-2.compute.internal, 35909, None) 19/11/19 04:43:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.39.147:53604) with ID 5 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ... (3) thriftserver restart \u00b6 Restart the thrift server. ec2-user@flashbase:1> thriftserver restart no org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 to stop starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out (4) start thriftserver \u00b6 Run the thrift server. ec2-user@flashbase:1> thriftserver start starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out You can view the logs through the command monitor . (5) stop thriftserver \u00b6 Shut down the thrift server. ec2-user@flashbase:1> thriftserver stop stopping org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 (6) conf thriftserver \u00b6 ec2-user@flashbase:1> conf thriftserver #!/bin/bash ############################################################################### # Common variables SPARK_CONF=${SPARK_CONF:-$SPARK_HOME/conf} SPARK_BIN=${SPARK_BIN:-$SPARK_HOME/bin} SPARK_SBIN=${SPARK_SBIN:-$SPARK_HOME/sbin} SPARK_LOG=${SPARK_LOG:-$SPARK_HOME/logs} SPARK_METRICS=${SPARK_CONF}/metrics.properties SPARK_UI_PORT=${SPARK_UI_PORT:-14050} EXECUTERS=12 EXECUTER_CORES=32 HIVE_METASTORE_URL='' HIVE_HOST=${HIVE_HOST:-localhost} HIVE_PORT=${HIVE_PORT:-13000} COMMON_CLASSPATH=$(find $SR2_LIB -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' \\ -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ':') ############################################################################### # Driver DRIVER_MEMORY=6g DRIVER_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Execute EXECUTOR_MEMORY=2g EXECUTOR_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Thrift Server logs EVENT_LOG_ENABLED=false EVENT_LOG_DIR=/nvdrive0/thriftserver-event-logs EVENT_LOG_ROLLING_DIR=/nvdrive0/thriftserver-event-logs-rolling EVENT_LOG_SAVE_MIN=60 EXTRACTED_EVENT_LOG_SAVE_DAY=5 SPARK_LOG_SAVE_MIN=2000 ############## If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9","title":"Commands"},{"location":"command-line-interface/#1-cluster-commands","text":"If you want to see the list of cluster commands, use the 'cluster' command without any option. ec2-user@flashbase:1> cluster NAME fbctl cluster - This is cluster command SYNOPSIS fbctl cluster COMMAND DESCRIPTION This is cluster command COMMANDS COMMAND is one of the following: add_slave Add slaves to cluster additionally clean Clean cluster configure create Create cluster ls Check cluster list rebalance Rebalance restart Restart redist cluster rowcount Query and show cluster row count start Start cluster stop Stop cluster use Change selected cluster","title":"1. Cluster Commands"},{"location":"command-line-interface/#1-cluster-configure","text":"redis-{port}.conf is generated with using redis-{master/slave}.conf.template and redis.properties. > cluster configure","title":"(1) cluster configure"},{"location":"command-line-interface/#2-cluster-start","text":"Backup logs of the previous master/slave nodes All log files of previous master/slave nodes in '${SR2_HOME} 1 /logs/redis/' will be moved to '${SR2_HOME}/logs/redis/backup/'. Generate directories to save data Save aof and rdb files of redis-server and RocksDB files in '${SR2_REDIS_DATA}' Start redis-server process Start master and slave redis-server with '${SR2_HOME}/conf/redis/redis-{port}.conf' file Log files will be saved in '${SR2_HOME}/logs/redis/' ec2-user@flashbase:1> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 Generate redis configuration files for master hosts sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ Starting master nodes : 127.0.0.1 : 18100|18101|18102|18103|18104 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up Errors ErrorCode 11 Redis-server(master) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... ... [ErrorCode 11] Fail to start... Must be checked running MASTER redis processes! We estimate that redis process is <alive-redis-count>. ErrorCode 12 Redis-server(slave) process with same port is already running. To resolve this error, use 'cluster stop' or 'kill {pid of the process}'. $ cluster start ... [ErrorCode 12] Fail to start... Must be checked running SLAVE redis processes! We estimate that redis process is <alive-redis-count>. Conf file not exist Conf file is not found. To resove this error, use 'cluster configure' and then 'cluster start'. cluster configure \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\uc2dc\ud0a8 \ud6c4 cluster start\ub97c \uc9c4\ud589\ud558\uc138\uc694. $ cluster start ... FileNotExistError: ${SR2_HOME}/conf/redis/redis-{port}.conf max try error \u200b For detail information, please check log files. $ cluster start ... max try error","title":"(2) cluster start"},{"location":"command-line-interface/#3-cluster-create","text":"After checking information of the cluster, create cluster of LightningDB. ec2-user@flashbase:1> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18100 - 127.0.0.1:18103 - 127.0.0.1:18104 - 127.0.0.1:18101 - 127.0.0.1:18102 Adding slots... - 127.0.0.1:18100, 3280 - 127.0.0.1:18103, 3276 - 127.0.0.1:18104, 3276 - 127.0.0.1:18101, 3276 - 127.0.0.1:18102, 3276 Check cluster state and asign slot... Ok create cluster complete. Errors When redis servers are not running, this error(Errno 111) will occur. To solve this error, use 'cluster start' command previously. ec2-user@flashbase:1> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18100 | MASTER | | 127.0.0.1 | 18101 | MASTER | | 127.0.0.1 | 18102 | MASTER | | 127.0.0.1 | 18103 | MASTER | | 127.0.0.1 | 18104 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y 127.0.0.1:18100 - [Errno 111] Connection refused \u200b\u200b","title":"(3) cluster create"},{"location":"command-line-interface/#4-cluster-stop","text":"\u200bGracefully kill all redis-servers(master/slave) with SIGINT \u200b\u200b ec2-user@flashbase:1> cluster stop Check status of hosts... OK Stopping master cluster of redis... cur: 5 / total: 5 cur: 0 / total: 5 Complete all redis process down Options Force to kill all redis-servers(master/slave) with SIGKILL --force","title":"(4) cluster stop"},{"location":"command-line-interface/#5-cluster-clean","text":"Remove conf files for redis-server and all data(aof, rdb, RocksDB) of LightningDB ec2-user@flashbase:1> cluster clean Removing redis generated master configuration files - 127.0.0.1 Removing flash db directory, appendonly and dump.rdb files in master - 127.0.0.1 Removing master node configuration - 127.0.0.1","title":"(5) cluster clean"},{"location":"command-line-interface/#6-cluster-restart","text":"Process 'cluster stop' and then 'cluster start'.\u200b\u200b Options Force to kill all redis-servers(master/slave) with SIGKILL and then start again. --force-stop Remove all data(aof, rdb, RocksDB, conf files) before start again. --reset Process 'cluster create'. This command should be called with '--reset'. --cluster","title":"(6) cluster restart\u200b"},{"location":"command-line-interface/#7-cluster-ls","text":"List up the deployed clusters. ec2-user@flashbase:2> cluster ls [1, 2]","title":"(7) cluster ls"},{"location":"command-line-interface/#8-cluster-use","text":"Change the cluster to use FBCTL. Use 'cluster use' or 'c' commands. ec2-user@flashbase:2> cluster use 1 Cluster '1' selected. ec2-user@flashbase:1> c 2 Cluster '2' selected.","title":"(8) cluster use"},{"location":"command-line-interface/#9-cluster-add_slave","text":"You can add a slave to a cluster that is configured only with master without redundancy. create cluster only with master Procedure for configuring the test environment. If cluster with only master already exists, go to the add slave info . Proceed with the deploy. Enter 0 in replicas as shown below when deploy. ec2-user@flashbase:2> deploy 3 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.5a6a38.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.5a6a38.bin Downloading flashbase.dev.master.5a6a38.bin [==================================================] 100% OK, flashbase.dev.master.5a6a38.bin Please type host list separated by comma(,) [127.0.0.1] OK, ['127.0.0.1'] How many masters would you like to create on each host? [5] OK, 5 Please type ports separate with comma(,) and use hyphen(-) for range. [18300-18304] OK, ['18300-18304'] How many replicas would you like to create on each master? [0] OK, 0 How many ssd would you like to use? [3] OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ +--------------+---------------------------------+ | NAME | VALUE | +--------------+---------------------------------+ | installer | flashbase.dev.master.5a6a38.bin | | hosts | 127.0.0.1 | | master ports | 18300-18304 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +--------------+---------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | CLEAN | +-----------+--------+ OK Transfer installer and execute... - 127.0.0.1 Sync conf... Complete to deploy cluster 3. Cluster '3' selected. When the deploy is complete, start and create the cluster. ec2-user@flashbase:3> cluster start Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK Backup redis master log in each MASTER hosts... - 127.0.0.1 create redis data directory in each MASTER hosts - 127.0.0.1 sync conf +-----------+--------+ | HOST | STATUS | +-----------+--------+ | 127.0.0.1 | OK | +-----------+--------+ OK Starting master nodes : 127.0.0.1 : 18300|18301|18302|18303|18304 ... Wait until all redis process up... cur: 5 / total: 5 Complete all redis process up ec2-user@flashbase:3> cluster create Check status of hosts... OK >>> Creating cluster +-----------+-------+--------+ | HOST | PORT | TYPE | +-----------+-------+--------+ | 127.0.0.1 | 18300 | MASTER | | 127.0.0.1 | 18301 | MASTER | | 127.0.0.1 | 18302 | MASTER | | 127.0.0.1 | 18303 | MASTER | | 127.0.0.1 | 18304 | MASTER | +-----------+-------+--------+ replicas: 0 Do you want to proceed with the create according to the above information? (y/n) y Cluster meet... - 127.0.0.1:18300 - 127.0.0.1:18303 - 127.0.0.1:18304 - 127.0.0.1:18301 - 127.0.0.1:18302 Adding slots... - 127.0.0.1:18300, 3280 - 127.0.0.1:18303, 3276 - 127.0.0.1:18304, 3276 - 127.0.0.1:18301, 3276 - 127.0.0.1:18302, 3276 Check cluster state and asign slot... Ok create cluster complete. ec2-user@flashbase:3> add slave info Open conf file. ec2-user@flashbase:3> conf cluster You can modify redis.properties by entering the command as shown above. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) #export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) #export SR2_REDIS_SLAVE_PORTS=( $(seq 18600 18609) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Modify SR2_REDIS_SLAVE_HOSTS and SR2_REDIS_SLAVE_PORTS as shown below. #!/bin/bash ## Master hosts and ports export SR2_REDIS_MASTER_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_MASTER_PORTS=( $(seq 18300 18304) ) ## Slave hosts and ports (optional) export SR2_REDIS_SLAVE_HOSTS=( \"127.0.0.1\" ) export SR2_REDIS_SLAVE_PORTS=( $(seq 18350 18354) ) ## only single data directory in redis db and flash db ## Must exist below variables; 'SR2_REDIS_DATA', 'SR2_REDIS_DB_PATH' and 'SR2_FLASH_DB_PATH' #export SR2_REDIS_DATA=\"/nvdrive0/nvkvs/redis\" #export SR2_REDIS_DB_PATH=\"/nvdrive0/nvkvs/redis\" #export SR2_FLASH_DB_PATH=\"/nvdrive0/nvkvs/flash\" ## multiple data directory in redis db and flash db export SSD_COUNT=3 #export HDD_COUNT=3 export SR2_REDIS_DATA=\"~/sata_ssd/ssd_\" export SR2_REDIS_DB_PATH=\"~/sata_ssd/ssd_\" export SR2_FLASH_DB_PATH=\"~/sata_ssd/ssd_\" ####################################################### # Example : only SSD data directory #export SSD_COUNT=3 #export SR2_REDIS_DATA=\"/ssd_\" #export SR2_REDIS_DB_PATH=\"/ssd_\" #export SR2_FLASH_DB_PATH=\"/ssd_\" ####################################################### Save the modification and exit. ec2-user@flashbase:3> conf cluster Check status of hosts... OK sync conf OK Complete edit execute command add-slave ec2-user@flashbase:3> cluster add-slave Check status of hosts... OK Check cluster exist... - 127.0.0.1 OK clean redis conf, node conf, db data of master clean redis conf, node conf, db data of slave - 127.0.0.1 Backup redis slave log in each SLAVE hosts... - 127.0.0.1 create redis data directory in each SLAVE hosts - 127.0.0.1 sync conf OK Starting slave nodes : 127.0.0.1 : 18350|18351|18352|18353|18354 ... Wait until all redis process up... cur: 10 / total: 10 Complete all redis process up replicate [M] 127.0.0.1 18300 - [S] 127.0.0.1 18350 replicate [M] 127.0.0.1 18301 - [S] 127.0.0.1 18351 replicate [M] 127.0.0.1 18302 - [S] 127.0.0.1 18352 replicate [M] 127.0.0.1 18303 - [S] 127.0.0.1 18353 replicate [M] 127.0.0.1 18304 - [S] 127.0.0.1 18354 1 / 5 meet complete. 2 / 5 meet complete. 3 / 5 meet complete. 4 / 5 meet complete. 5 / 5 meet complete. check configuration information ec2-user@flashbase:3> cli cluster nodes 0549ec03031213f95121ceff6c9c13800aef848c 127.0.0.1:18303 master - 0 1574132251126 3 connected 3280-6555 1b09519d37ebb1c09095158b4f1c9f318ddfc747 127.0.0.1:18352 slave a6a8013cf0032f0f36baec3162122b3d993dd2c8 0 1574132251025 6 connected c7dc4815e24054104dff61cac6b13256a84ac4ae 127.0.0.1:18353 slave 0549ec03031213f95121ceff6c9c13800aef848c 0 1574132251126 3 connected 0ab96cb79165ddca7d7134f80aea844bd49ae2e1 127.0.0.1:18351 slave 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 0 1574132250724 4 connected 7e97f8a8799e1e28feee630b47319e6f5e1cfaa7 127.0.0.1:18301 master - 0 1574132250524 4 connected 9832-13107 e67005a46984445e559a1408dd0a4b24a8c92259 127.0.0.1:18304 master - 0 1574132251126 5 connected 6556-9831 a6a8013cf0032f0f36baec3162122b3d993dd2c8 127.0.0.1:18302 master - 0 1574132251126 2 connected 13108-16383 492cdf4b1dedab5fb94e7129da2a0e05f6c46c4f 127.0.0.1:18350 slave 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 0 1574132251126 6 connected f9f7fcee9009f25618e63d2771ee2529f814c131 127.0.0.1:18354 slave e67005a46984445e559a1408dd0a4b24a8c92259 0 1574132250724 5 connected 83b7ef98b80a05a4ee795ae6b399c8cde54ad04e 127.0.0.1:18300 myself,master - 0 1574132250000 1 connected 0-3279","title":"(9) cluster add_slave"},{"location":"command-line-interface/#10-cluster-rowcount","text":"Check the count of records that are stored in the cluster. ec2-user@flashbase:1> cluster rowcount 0","title":"(10) cluster rowcount"},{"location":"command-line-interface/#11-check-status-of-cluster","text":"With the following commands, you can check the status of the cluster. Send PING > cli ping --all Check the status of the cluster ec2-user@flashbase:1> cli cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:5 cluster_size:5 cluster_current_epoch:4 cluster_my_epoch:2 cluster_stats_messages_ping_sent:12 cluster_stats_messages_pong_sent:14 cluster_stats_messages_sent:26 cluster_stats_messages_ping_received:10 cluster_stats_messages_pong_received:12 cluster_stats_messages_meet_received:4 cluster_stats_messages_received:26 Check the list of the nodes those are organizing the cluster. ec2-user@flashbase:1> cli cluster nodes 559af5e90c3f2c92f19c927c29166c268d938e8f 127.0.0.1:18104 master - 0 1574127926000 4 connected 6556-9831 174e2a62722273fb83814c2f12e2769086c3d185 127.0.0.1:18101 myself,master - 0 1574127925000 3 connected 9832-13107 35ab4d3f7f487c5332d7943dbf4b20d5840053ea 127.0.0.1:18100 master - 0 1574127926000 1 connected 0-3279 f39ed05ace18e97f74c745636ea1d171ac1d456f 127.0.0.1:18103 master - 0 1574127927172 0 connected 3280-6555 9fd612b86a9ce1b647ba9170b8f4a8bfa5c875fc 127.0.0.1:18102 master - 0 1574127926171 2 connected 13108-16383","title":"(11) Check status of cluster"},{"location":"command-line-interface/#2-thrift-server-commands","text":"If you want to see the list of Thrift Server commands, use the 'thriftserver' command without any option. NAME fbctl thriftserver SYNOPSIS fbctl thriftserver COMMAND COMMANDS COMMAND is one of the following: beeline Connect to thriftserver command line monitor Show thriftserver log restart Thriftserver restart start Start thriftserver stop Stop thriftserver","title":"2. Thrift Server Commands"},{"location":"command-line-interface/#1-thriftserver-beeline","text":"Connect to the thrift server ec2-user@flashbase:1> thriftserver beeline Connecting... Connecting to jdbc:hive2://localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Supplied authorities: localhost:13000 19/11/19 04:45:18 INFO jdbc.Utils: Resolved authority: localhost:13000 19/11/19 04:45:18 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:13000 Connected to: Spark SQL (version 2.3.1) Driver: Hive JDBC (version 1.2.1.spark2) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 1.2.1.spark2 by Apache Hive 0: jdbc:hive2://localhost:13000> show tables; +-----------+------------+--------------+--+ | database | tableName | isTemporary | +-----------+------------+--------------+--+ +-----------+------------+--------------+--+ No rows selected (0.55 seconds) Default value of db url to connect is jdbc:hive2://$HIVE_HOST:$HIVE_PORT You can modify $HIVE_HOST and $HIVE_PORT by command conf ths","title":"(1) thriftserver beeline"},{"location":"command-line-interface/#2-thriftserver-monitor","text":"You can view the logs of the thrift server in real time. ec2-user@flashbase:1> thriftserver monitor Press Ctrl-C for exit. 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-172-31-39-147.ap-northeast-2.compute.internal:35909 with 912.3 MB RAM, BlockManagerId(4, ip-172-31-39-147.ap-northeast-2.compute.internal, 35909, None) 19/11/19 04:43:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.39.147:53604) with ID 5 19/11/19 04:43:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ...","title":"(2) thriftserver monitor"},{"location":"command-line-interface/#3-thriftserver-restart","text":"Restart the thrift server. ec2-user@flashbase:1> thriftserver restart no org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 to stop starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out","title":"(3) thriftserver restart"},{"location":"command-line-interface/#4-start-thriftserver","text":"Run the thrift server. ec2-user@flashbase:1> thriftserver start starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark-ec2-user-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-ip-172-31-39-147.ap-northeast-2.compute.internal.out You can view the logs through the command monitor .","title":"(4) start thriftserver"},{"location":"command-line-interface/#5-stop-thriftserver","text":"Shut down the thrift server. ec2-user@flashbase:1> thriftserver stop stopping org.apache.spark.sql.hive.thriftserver.HiveThriftServer2","title":"(5) stop thriftserver"},{"location":"command-line-interface/#6-conf-thriftserver","text":"ec2-user@flashbase:1> conf thriftserver #!/bin/bash ############################################################################### # Common variables SPARK_CONF=${SPARK_CONF:-$SPARK_HOME/conf} SPARK_BIN=${SPARK_BIN:-$SPARK_HOME/bin} SPARK_SBIN=${SPARK_SBIN:-$SPARK_HOME/sbin} SPARK_LOG=${SPARK_LOG:-$SPARK_HOME/logs} SPARK_METRICS=${SPARK_CONF}/metrics.properties SPARK_UI_PORT=${SPARK_UI_PORT:-14050} EXECUTERS=12 EXECUTER_CORES=32 HIVE_METASTORE_URL='' HIVE_HOST=${HIVE_HOST:-localhost} HIVE_PORT=${HIVE_PORT:-13000} COMMON_CLASSPATH=$(find $SR2_LIB -name 'tsr2*' -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' \\ -o -name 'geospark*' -o -name 'gt-*' | tr '\\n' ':') ############################################################################### # Driver DRIVER_MEMORY=6g DRIVER_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Execute EXECUTOR_MEMORY=2g EXECUTOR_CLASSPATH=$COMMON_CLASSPATH ############################################################################### # Thrift Server logs EVENT_LOG_ENABLED=false EVENT_LOG_DIR=/nvdrive0/thriftserver-event-logs EVENT_LOG_ROLLING_DIR=/nvdrive0/thriftserver-event-logs-rolling EVENT_LOG_SAVE_MIN=60 EXTRACTED_EVENT_LOG_SAVE_DAY=5 SPARK_LOG_SAVE_MIN=2000 ############## If user types 'cfc 1', ${SR2_HOME} will be '~/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT'. \u21a9","title":"(6) conf thriftserver"},{"location":"data-ingestion-and-querying/","text":"1. Table Reads & Writes \u00b6 Create a table \u00b6 You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". df.write.format(\"r2\").insertInto(r2TableName) 2. Data Ingestion and Querying \u00b6 3. Querying \u00b6","title":"Data Ingestion and Querying"},{"location":"data-ingestion-and-querying/#1-table-reads-writes","text":"","title":"1. Table Reads &amp; Writes"},{"location":"data-ingestion-and-querying/#create-a-table","text":"You can use DataFrameWriter to write data into LightningDB. Now, LightingDB only supports \" Append mode \". df.write.format(\"r2\").insertInto(r2TableName)","title":"Create a table"},{"location":"data-ingestion-and-querying/#2-data-ingestion-and-querying","text":"","title":"2. Data Ingestion and Querying"},{"location":"data-ingestion-and-querying/#3-querying","text":"","title":"3. Querying"},{"location":"eula/","text":"LightningDB - EULA \u00b6 End\u00adUser License Agreement (\"Agreement\") \u00b6 Last updated: (Sep. 26. 2019) Please read this End\u00adUser License Agreement (\"Agreement\") carefully before clicking the \"Subscribing\" button, or using LightningDB\u200b(\"Application\"). By clicking the \"Subscribe\" button, or using the Application, you are agreeing to be bound by the terms and conditions of this Agreement. If you do not agree to the terms of this Agreement, do not click on the \"Subscribe\" button and do not subscribe or use the Application. License \u00b6 SK Telecom \u200bgrants you a revocable, non\u00adexclusive, non\u00adtransferable, limited license to subscribe and use the Application solely for your personal, non\u00adcommercial purposes strictly in accordance with the terms of this Agreement. Restrictions \u00b6 You agree not to, and you will not permit others to: a) license, sell, rent, lease, assign, distribute, transmit, host, outsource, disclose or otherwise commercially exploit the Application or make the Application available to any third party. Modifications to Application \u00b6 SK Telecom \u200breserves the right to modify, suspend or discontinue, temporarily or permanently, the Application or any service to which it connects, with or without notice and without liability to you. Term and Termination \u00b6 This Agreement shall remain in effect until terminated by you or SK Telecom.\u200b SK Telecom \u200bmay, in its sole discretion, at any time and for any or no reason, suspend or terminate this Agreement with or without prior notice. This Agreement will terminate immediately, without prior notice from SK Telecom,\u200bin the event that you fail to comply with any provision of this Agreement. You may also terminate this Agreement by unsubscribing the Application. Upon termination of this Agreement, you shall cease all use of the Application and delete all copies of the Application from your device or from your cloud. Severability \u00b6 If any provision of this Agreement is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect. Amendments to this Agreement \u00b6 SK Telecom \u200breserves the right, at its sole discretion, to modify or replace this Agreement at any time. If a revision is material we will provide at least 7 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion. Contact Information \u00b6 If you have any questions about this Agreement, please contact us.","title":"LightningDB - EULA"},{"location":"eula/#lightningdb-eula","text":"","title":"LightningDB - EULA"},{"location":"eula/#enduser-license-agreement-agreement","text":"Last updated: (Sep. 26. 2019) Please read this End\u00adUser License Agreement (\"Agreement\") carefully before clicking the \"Subscribing\" button, or using LightningDB\u200b(\"Application\"). By clicking the \"Subscribe\" button, or using the Application, you are agreeing to be bound by the terms and conditions of this Agreement. If you do not agree to the terms of this Agreement, do not click on the \"Subscribe\" button and do not subscribe or use the Application.","title":"End\u00adUser License Agreement (\"Agreement\")"},{"location":"eula/#license","text":"SK Telecom \u200bgrants you a revocable, non\u00adexclusive, non\u00adtransferable, limited license to subscribe and use the Application solely for your personal, non\u00adcommercial purposes strictly in accordance with the terms of this Agreement.","title":"License"},{"location":"eula/#restrictions","text":"You agree not to, and you will not permit others to: a) license, sell, rent, lease, assign, distribute, transmit, host, outsource, disclose or otherwise commercially exploit the Application or make the Application available to any third party.","title":"Restrictions"},{"location":"eula/#modifications-to-application","text":"SK Telecom \u200breserves the right to modify, suspend or discontinue, temporarily or permanently, the Application or any service to which it connects, with or without notice and without liability to you.","title":"Modifications to Application"},{"location":"eula/#term-and-termination","text":"This Agreement shall remain in effect until terminated by you or SK Telecom.\u200b SK Telecom \u200bmay, in its sole discretion, at any time and for any or no reason, suspend or terminate this Agreement with or without prior notice. This Agreement will terminate immediately, without prior notice from SK Telecom,\u200bin the event that you fail to comply with any provision of this Agreement. You may also terminate this Agreement by unsubscribing the Application. Upon termination of this Agreement, you shall cease all use of the Application and delete all copies of the Application from your device or from your cloud.","title":"Term and Termination"},{"location":"eula/#severability","text":"If any provision of this Agreement is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.","title":"Severability"},{"location":"eula/#amendments-to-this-agreement","text":"SK Telecom \u200breserves the right, at its sole discretion, to modify or replace this Agreement at any time. If a revision is material we will provide at least 7 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.","title":"Amendments to this Agreement"},{"location":"eula/#contact-information","text":"If you have any questions about this Agreement, please contact us.","title":"Contact Information"},{"location":"get-started-with-aws/","text":"Note This page guides how to start LightningDB automatically only for the case of AWS EC2 Instance . 1. Access EC2 Instance \u00b6 Create EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS} 2. Script for setup environment \u00b6 After access EC2 Instance, run script to setup environment. $ cd ~/flashbase/scripts/userdata/per-boot $ ./run.sh After 'run.sh' is completed, following jobs are done. Create and exchange SSH KEY for user authentication Mount disks Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration 3. Start LightningDB \u00b6 LightningDB provides fbctl that is introduced in Install and Upgrade . With fbctl , user can deploy and use LightningDB. LightningDB supports Zeppelin to provide convenience of ingestion and querying data of LightningDB. About Zeppelin , Data Ingestion and Querying page provides some examples.","title":"Automated Installation (recommended)"},{"location":"get-started-with-aws/#1-access-ec2-instance","text":"Create EC2 Instance for LightningDB and access with 'Public IP' or 'Public DNS'. '*.pem' file is also required to access EC2 Instance. $ ssh -i /path/to/.pem ec2-user@${IP_ADDRESS}","title":"1. Access EC2 Instance"},{"location":"get-started-with-aws/#2-script-for-setup-environment","text":"After access EC2 Instance, run script to setup environment. $ cd ~/flashbase/scripts/userdata/per-boot $ ./run.sh After 'run.sh' is completed, following jobs are done. Create and exchange SSH KEY for user authentication Mount disks Set Hadoop configurations(core-site.xml, hdfs-site.xml, yarn-site.xml). This settings is default value for starter of Hadoop. To optimize resource or performance, user needs to modify some features with Hadoop Get Started Set Spark configuration(spark-default.conf.template) To optimize resource and performance, user also need to modify some features with Spark Configuration","title":"2. Script for setup environment"},{"location":"get-started-with-aws/#3-start-lightningdb","text":"LightningDB provides fbctl that is introduced in Install and Upgrade . With fbctl , user can deploy and use LightningDB. LightningDB supports Zeppelin to provide convenience of ingestion and querying data of LightningDB. About Zeppelin , Data Ingestion and Querying page provides some examples.","title":"3. Start LightningDB"},{"location":"get-started-with-scratch/","text":"Note This page guides how to start LightningDB on CentOS manually. In case of using AWS EC2 Instance , please use 1. Automated Installation (recommanded) 1. Optimizing System Parameters \u00b6 (1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages 2. Setup Prerequisites \u00b6 bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install go to the website: https://software.intel.com/en-us/mkl/choose-download/macos register and login select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks. 3. Session configuration files \u00b6 Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster' 4. Install and Start LightningDB \u00b6 With fbctl provided by LightningDB, user can deploy and use LightningDB. Install fbctl with following command. $ pip insatll fbctl After installation is completed, start fbctl with Commands","title":"Manual Installation"},{"location":"get-started-with-scratch/#1-optimizing-system-parameters","text":"(1) Edit /etc/sysctl.conf like following ... vm.swappiness = 0 vm.overcommit_memory = 1 vm.overcommit_ratio = 50 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 32768 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_max_syn_backlog = 4096 net.core.somaxconn = 65535 ... Tip In case of application in runtime, use sudo sysctl -p (2) Edit /etc/security/limits.conf ... * soft nofile 262144 * hard nofile 262144 * soft nproc 131072 * hard nproc 131072 [account name] * soft nofile 262144 [account name] * hard nofile 262144 [account name] * soft nproc 131072 [account name] * hard nproc 131072 ... Tip In case of application in runtime, use ulimit -n 65535, ulimit -u 131072 (3) Edit /etc/fstab Remove SWAP Partition (Comment out SWAP partition with using # and reboot) ... #/dev/mapper/centos-swap swap swap defaults 0 0 ... Tip In case of application in runtime, use swapoff -a (4) /etc/init.d/disable-transparent-hugepages root@fbg01 ~] cat /etc/init.d/disable-transparent-hugepages #!/bin/bash ### BEGIN INIT INFO # Provides: disable-transparent-hugepages # Required-Start: $local_fs # Required-Stop: # X-Start-Before: mongod mongodb-mms-automation-agent # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Disable Linux transparent huge pages # Description: Disable Linux transparent huge pages, to improve # database performance. ### END INIT INFO case $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' > ${thp_path}/enabled echo 'never' > ${thp_path}/defrag re='^[0-1]+$' if [[ $(cat ${thp_path}/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 > ${thp_path}/khugepaged/defrag else # RHEL 6 echo 'no' > ${thp_path}/khugepaged/defrag fi unset re unset thp_path ;; esac [root@fbg01 ~] [root@fbg01 ~] [root@fbg01 ~] chmod 755 /etc/init.d/disable-transparent-hugepages [root@fbg01 ~] chkconfig --add disable-transparent-hugepages","title":"1. Optimizing System Parameters"},{"location":"get-started-with-scratch/#2-setup-prerequisites","text":"bash, unzip, ssh JDK 1.8 or higher gcc 4.8.5 or higher glibc 2.17 or higher epel-release sudo yum install epel-release boost, boost-thread, boost-devel sudo yum install boost boost-thread boost-devel Exchange SSH Key For all servers that LightningDB will be deployed, SSH key should be exchanged. ssh-keygen -t rsa chmod 0600 ~/.ssh/authorized_keys cat .ssh/id_rsa.pub | ssh {server name} \"cat >> .ssh/authorized_keys\" Intel MKL library (1) Intel MKL 2019 library install go to the website: https://software.intel.com/en-us/mkl/choose-download/macos register and login select product named \"Intel * Math Kernel Library for Linux\" or \"Intel * Math Kernel Library for Mac\" from the select box \"Choose Product to Download\" Choose a Version \"2019 Update 2\" and download unzip the file and execute the install.sh file with root account or (sudo command) sudo ./install.sh choose custom install and configure the install directory /opt/intel (with sudo, /opt/intel is the default installation path, just confirm it) matthew@fbg05 /opt/intel $ pwd /opt/intel matthew@fbg05 /opt/intel $ ls -alh \ud569\uacc4 0 drwxr-xr-x 10 root root 307 3\uc6d4 22 01:34 . drwxr-xr-x. 5 root root 83 3\uc6d4 22 01:34 .. drwxr-xr-x 6 root root 72 3\uc6d4 22 01:35 .pset drwxr-xr-x 2 root root 53 3\uc6d4 22 01:34 bin lrwxrwxrwx 1 root root 28 3\uc6d4 22 01:34 compilers_and_libraries -> compilers_and_libraries_2019 drwxr-xr-x 3 root root 19 3\uc6d4 22 01:34 compilers_and_libraries_2019 drwxr-xr-x 4 root root 36 1\uc6d4 24 23:04 compilers_and_libraries_2019.2.187 drwxr-xr-x 6 root root 63 1\uc6d4 24 22:50 conda_channel drwxr-xr-x 4 root root 26 1\uc6d4 24 23:01 documentation_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 lib -> compilers_and_libraries/linux/lib lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 mkl -> compilers_and_libraries/linux/mkl lrwxrwxrwx 1 root root 29 3\uc6d4 22 01:34 parallel_studio_xe_2019 -> parallel_studio_xe_2019.2.057 drwxr-xr-x 5 root root 216 3\uc6d4 22 01:34 parallel_studio_xe_2019.2.057 drwxr-xr-x 3 root root 16 3\uc6d4 22 01:34 samples_2019 lrwxrwxrwx 1 root root 33 3\uc6d4 22 01:34 tbb -> compilers_and_libraries/linux/tbb (2) Intel MKL 2019 library environment settings append the following statement into ~/.bashrc # INTEL MKL enviroment variables for ($MKLROOT, can be checked with the value export | grep MKL) source /opt/intel/mkl/bin/mklvars.sh intel64 Apache Hadoop 2.6.0 (or higher) Apache Spark 2.3 on Hadoop 2.6 ntp: For clock synchronization between servers over packet-switched, variable-latency data networks.","title":"2. Setup Prerequisites"},{"location":"get-started-with-scratch/#3-session-configuration-files","text":"Edit ~/.bashrc Add followings # .bashrc if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/.local/bin:$HOME/bin HADOOP_HOME=/home/nvkvs/hadoop HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop SPARK_HOME=/home/nvkvs/spark PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HOME/sbin export PATH SPARK_HOME HADOOP_HOME HADOOP_CONF_DIR YARN_CONF_DIR alias cfc='source ~/.use_cluster'","title":"3. Session configuration files"},{"location":"get-started-with-scratch/#4-install-and-start-lightningdb","text":"With fbctl provided by LightningDB, user can deploy and use LightningDB. Install fbctl with following command. $ pip insatll fbctl After installation is completed, start fbctl with Commands","title":"4. Install and Start LightningDB"},{"location":"release-note/","text":"1. Recommended Versions \u00b6 LightningDB ver 1.0 2. Release Notes \u00b6 Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions","title":"Release Notes"},{"location":"release-note/#1-recommended-versions","text":"LightningDB ver 1.0","title":"1. Recommended Versions"},{"location":"release-note/#2-release-notes","text":"Ver 1.0 Date: 2019.11.20 Download: LightningDB ver 1.0 License: free Description Initial version Support FBCTL Support geoSpatial functions","title":"2. Release Notes"},{"location":"run-and-deploy-fbctl/","text":"Note Command Line Interface(CLI) of LightningDB supports not only deploy and start command but also many commands to access and manipulate data in LightningDB. \u200b 1. How to run fbctl \u00b6 If you try to use fbctl for the first time after EC2 instance was created, please update fbctl like below. pip install fbctl --upgrade --user (1) Run \u00b6 To run fbctl, ${FBPATH} should be set. If not, following error messages will be shown. To start using fbctl, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In case of EC2 Instance, this path is set automatically. Run fbctl by typing 'fbctl' $ fbcli When fbctl starts at the first time, user needs to confirm 'base_directory'. [~/tsr2] 1 ] is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', user can modify 'base_directory'. If user logs in fbctl normally, fbctl starts on last visited cluster. In case of first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that user is in Centos and '>' means that user is in fbctl. (2) Log messages \u00b6 Log messages of fbctl will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceed of size limit. 2. Deploy LightningDB \u00b6 Deploy is the procedure that LightningDB is installed with the specified cluster number. User could make LightningDB cluster with the following command. > deploy 1 After deploy command, user should type the following information that provides its last used value. installer host number of masters replicas number of ssd(disk) prefix of (redis data / redis db path / flash db path) Use below option not to save last used value. > deploy --history-save=False (1) Select installer \u00b6 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin With only URL, instead of file path, LightningDB can be installed like below. To copy the link of the recommended FlashBase version, use Release Notes . https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.dbcb9e.bin Downloading flashbase.dev.master.dbcb9e.bin [======= ] 15% (2) Type Hosts \u00b6 IP address or hostname can be used. In case of several hosts, list can be seperated by comma(','). Please type host list separated by comma(,) [127.0.0.1] nodeA, nodeB, nodeC, nodeD OK, ['nodeA', 'nodeB', 'nodeC', 'nodeD'] (3) Type Masters \u00b6 How many masters would you like to create on each host? [1] 1 OK, 1 Please type ports separate with comma(,) and use hyphen(-) for range. [18100] 18100 OK, ['18100'] Define how many master processes will be created in the cluster per server. (4) Type information of slave \u00b6 How many replicas would you like to create on each master? [2] 2 OK, 2 Please type ports separate with comma(,) and use hyphen(-) for range. [18150-18151] 18150-18151 OK, ['18150-18151'] Define how many slave processes will be created for a master process. (5) Type the count of SSD(disk) and the path of DB files \u00b6 How many sdd would you like to use? [3] 3 OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_ (6) Check all settings finally \u00b6 Finally all settings will be shown and confirmation will be requested like below. +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y (7) Deploy cluster \u00b6 After deploying is completed, following messages are shown and fbctl of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLEAN | | nodeD | CLEAN | +-----------+--------+ OK Transfer install and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1 Cluster 1 selected. When an error occurs during deploying, error messages will be shown like below. Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If localhost(127.0.0.1) is not included in host information, this error occurs. Please add localhost in host list in this case. From now, user can start and manage clusters of LightningDB with Commands . 3. LightningDB Version Update \u00b6 In case of version update, 'deploy' command is used. > c 1 // alias of 'cluster use 1' > deploy (Watch out) Cluster 1 is already deployed. Do you want to deploy again? (y/n) [n] y (1) Select installer \u00b6 Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin (2) Restore \u00b6 Do you want to restore conf? (y/n) y If the current settings will be reused, type 'y'. (3) Check all settings finally \u00b6 +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | redis data path | ~/sata_ssd/ssd_ | | redis db path | ~/sata_ssd/ssd_ | | flash db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ Checking for cluster exist... +------+--------+ | HOST | STATUS | +------+--------+ Backup conf of cluster 1... OK, cluster_1_conf_bak_<time-stamp> Backup info of cluster 1 at nodeA... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeB... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeC... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeD... OK, cluster_1_bak_<time-stamp> Transfer installer and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1. Cluster 1 selected. Backup path of cluster: ${base-directory}/backup/cluster_${cluster-id}_bak_${time-stamp} Backup path of conf files: $FBAPTH/conf_backup/cluster_${cluster-id}_conf_bak_${time-stamp} (4) Restart \u00b6 > cluster restart After restart, new version will be applied. If user types 'enter' without any text, the default value is applied. In some case, default value will not provided. \u21a9","title":"Install and Upgrade"},{"location":"run-and-deploy-fbctl/#1-how-to-run-fbctl","text":"If you try to use fbctl for the first time after EC2 instance was created, please update fbctl like below. pip install fbctl --upgrade --user","title":"1. How to run fbctl"},{"location":"run-and-deploy-fbctl/#1-run","text":"To run fbctl, ${FBPATH} should be set. If not, following error messages will be shown. To start using fbctl, you should set env FBPATH ex) export FBPATH=$HOME/.flashbase Tip In case of EC2 Instance, this path is set automatically. Run fbctl by typing 'fbctl' $ fbcli When fbctl starts at the first time, user needs to confirm 'base_directory'. [~/tsr2] 1 ] is default value. Type base directory of flashbase [~/tsr2] ~/tsr2 OK, ~/tsr2 In '${FBPATH}/.flashbase/config', user can modify 'base_directory'. If user logs in fbctl normally, fbctl starts on last visited cluster. In case of first login, '-' is shown instead of cluster number. root@flashbase:-> ... ... root@flashbase:1> Tip In this page, '$' means that user is in Centos and '>' means that user is in fbctl.","title":"(1) Run"},{"location":"run-and-deploy-fbctl/#2-log-messages","text":"Log messages of fbctl will be saved in '$FBPATH/logs/fb-roate.log'. Its max-file-size is 1GiB and rolling update will be done in case of exceed of size limit.","title":"(2) Log messages"},{"location":"run-and-deploy-fbctl/#2-deploy-lightningdb","text":"Deploy is the procedure that LightningDB is installed with the specified cluster number. User could make LightningDB cluster with the following command. > deploy 1 After deploy command, user should type the following information that provides its last used value. installer host number of masters replicas number of ssd(disk) prefix of (redis data / redis db path / flash db path) Use below option not to save last used value. > deploy --history-save=False","title":"2. Deploy LightningDB"},{"location":"run-and-deploy-fbctl/#1-select-installer","text":"Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin With only URL, instead of file path, LightningDB can be installed like below. To copy the link of the recommended FlashBase version, use Release Notes . https://flashbase.s3.ap-northeast-2.amazonaws.com/flashbase.dev.master.dbcb9e.bin Downloading flashbase.dev.master.dbcb9e.bin [======= ] 15%","title":"(1) Select installer"},{"location":"run-and-deploy-fbctl/#2-type-hosts","text":"IP address or hostname can be used. In case of several hosts, list can be seperated by comma(','). Please type host list separated by comma(,) [127.0.0.1] nodeA, nodeB, nodeC, nodeD OK, ['nodeA', 'nodeB', 'nodeC', 'nodeD']","title":"(2) Type Hosts"},{"location":"run-and-deploy-fbctl/#3-type-masters","text":"How many masters would you like to create on each host? [1] 1 OK, 1 Please type ports separate with comma(,) and use hyphen(-) for range. [18100] 18100 OK, ['18100'] Define how many master processes will be created in the cluster per server.","title":"(3) Type Masters"},{"location":"run-and-deploy-fbctl/#4-type-information-of-slave","text":"How many replicas would you like to create on each master? [2] 2 OK, 2 Please type ports separate with comma(,) and use hyphen(-) for range. [18150-18151] 18150-18151 OK, ['18150-18151'] Define how many slave processes will be created for a master process.","title":"(4) Type information of slave"},{"location":"run-and-deploy-fbctl/#5-type-the-count-of-ssddisk-and-the-path-of-db-files","text":"How many sdd would you like to use? [3] 3 OK, 3 Type prefix of db path [~/sata_ssd/ssd_] OK, ~/sata_ssd/ssd_","title":"(5) Type the count of SSD(disk) and the path of DB files"},{"location":"run-and-deploy-fbctl/#6-check-all-settings-finally","text":"Finally all settings will be shown and confirmation will be requested like below. +-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y","title":"(6) Check all settings finally"},{"location":"run-and-deploy-fbctl/#7-deploy-cluster","text":"After deploying is completed, following messages are shown and fbctl of the cluster is activated. Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ OK Checking for cluster exist... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLEAN | | nodeD | CLEAN | +-----------+--------+ OK Transfer install and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1 Cluster 1 selected. When an error occurs during deploying, error messages will be shown like below. Host connection error Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeA | OK | | nodeB | SSH ERROR | | nodeC | UNKNOWN HOST | | nodeD | CONNECTION ERROR | +-------+------------------+ There are unavailable host. SSH ERROR SSH access error. Please check SSH KEY exchange or the status of SSH client/server. UNKNOWN HOST Can not get IP address with the hostname. Please check if the hostname is right. CONNECTION ERROR Please check the status of host(server) or outbound/inbound of the server. Cluster already exist Checking for cluster exist... +-------+---------------+ | HOST | STATUS | +-------+---------------+ | nodeA | CLEAN | | nodeB | CLEAN | | nodeC | CLUSTER EXIST | | nodeD | CLUSTER EXIST | +-------+---------------+ Cluster information exist on some hosts. CLUSTER EXIST LightningDB is already deployed in the cluster of the host. Not include localhost Check status of hosts... +-------+------------------+ | HOST | STATUS | +-------+------------------+ | nodeB | OK | | nodeC | OK | | nodeD | OK | +-------+------------------+ Must include localhost. If localhost(127.0.0.1) is not included in host information, this error occurs. Please add localhost in host list in this case. From now, user can start and manage clusters of LightningDB with Commands .","title":"(7) Deploy cluster"},{"location":"run-and-deploy-fbctl/#3-lightningdb-version-update","text":"In case of version update, 'deploy' command is used. > c 1 // alias of 'cluster use 1' > deploy (Watch out) Cluster 1 is already deployed. Do you want to deploy again? (y/n) [n] y","title":"3. LightningDB Version Update"},{"location":"run-and-deploy-fbctl/#1-select-installer_1","text":"Select installer [ INSTALLER LIST ] (1) flashbase.dev.master.dbcb9e.bin (2) flashbase.trial.master.dbcb9e-dirty.bin (3) flashbase.trial.master.dbcb9e.bin Please enter the number, file path or url of the installer you want to use. you can also add file in list by copy to '$FBPATH/releases/' 1 OK, flashbase.dev.master.dbcb9e.bin","title":"(1) Select installer"},{"location":"run-and-deploy-fbctl/#2-restore","text":"Do you want to restore conf? (y/n) y If the current settings will be reused, type 'y'.","title":"(2) Restore"},{"location":"run-and-deploy-fbctl/#3-check-all-settings-finally","text":"+-----------------+---------------------------------------------+ | NAME | VALUE | +-----------------+---------------------------------------------+ | installer | flashbase.dev.master.dbcb9e.bin | | nodes | nodeA | | | nodeB | | | nodeC | | | nodeD | | master ports | 18100 | | slave ports | 18150-18151 | | ssd count | 3 | | redis data path | ~/sata_ssd/ssd_ | | redis db path | ~/sata_ssd/ssd_ | | flash db path | ~/sata_ssd/ssd_ | +-----------------+---------------------------------------------+ Do you want to proceed with the deploy accroding to the above information? (y/n) y Check status of hosts... +-----------+--------+ | HOST | STATUS | +-----------+--------+ | nodeA | OK | | nodeB | OK | | nodeC | OK | | nodeD | OK | +-----------+--------+ Checking for cluster exist... +------+--------+ | HOST | STATUS | +------+--------+ Backup conf of cluster 1... OK, cluster_1_conf_bak_<time-stamp> Backup info of cluster 1 at nodeA... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeB... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeC... OK, cluster_1_bak_<time-stamp> Backup info of cluster 1 at nodeD... OK, cluster_1_bak_<time-stamp> Transfer installer and execute... - nodeA - nodeB - nodeC - nodeD Sync conf... Complete to deploy cluster 1. Cluster 1 selected. Backup path of cluster: ${base-directory}/backup/cluster_${cluster-id}_bak_${time-stamp} Backup path of conf files: $FBAPTH/conf_backup/cluster_${cluster-id}_conf_bak_${time-stamp}","title":"(3) Check all settings finally"},{"location":"run-and-deploy-fbctl/#4-restart","text":"> cluster restart After restart, new version will be applied. If user types 'enter' without any text, the default value is applied. In some case, default value will not provided. \u21a9","title":"(4) Restart"},{"location":"try-with-zeppelin/","text":"1. Setting for Zeppelin \u00b6 You can try LightningDB in Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using fbctl before launching Zeppelin daemon. Secondly, in order to run LightingDB on the Spark, the jars in the LightingDB should be passed to the Spark. This can be done by adjusting SPARK_SUBMIT_OPTIONS in zeppllin-env.sh $ cp $ZEPPELIN_HOME/conf/zeppelin-env.sh.template $ZEPPELIN_HOME/conf/zeppelin-env.sh $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh # /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT is a path in which LightningDB is installed using fbctl. # This can be different if you installed LightningDB in different path. export SPARK_SUBMIT_OPTIONS=\"--jars $(find /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT/lib -name 'tsr2*' \\ -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' \\ -o -name 'gt-*' | tr '\\n' ',')\" Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start 2. Tutorial with Zeppelin \u00b6 After starting zeppelin daemon, you can access zeppelin UI using browser. The url is https://your-server-ip:8080 . There is a github page for tutorial . The repository includes a tool for generating sample data and a notebook for tutorial. You can import the tutorial notebook with its url. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of Spark driver is at least 10GB in Spark interpreter setting. Also, make sure that the timeout of shell command is at least 120000 ms.","title":"Try out with Zeppelin"},{"location":"try-with-zeppelin/#1-setting-for-zeppelin","text":"You can try LightningDB in Zeppelin notebook. Firstly, deploy and start the cluster of LightningDB using fbctl before launching Zeppelin daemon. Secondly, in order to run LightingDB on the Spark, the jars in the LightingDB should be passed to the Spark. This can be done by adjusting SPARK_SUBMIT_OPTIONS in zeppllin-env.sh $ cp $ZEPPELIN_HOME/conf/zeppelin-env.sh.template $ZEPPELIN_HOME/conf/zeppelin-env.sh $ vim $ZEPPELIN_HOME/conf/zeppelin-env.sh # /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT is a path in which LightningDB is installed using fbctl. # This can be different if you installed LightningDB in different path. export SPARK_SUBMIT_OPTIONS=\"--jars $(find /home/ec2-user/tsr2/cluster_1/tsr2-assembly-1.0.0-SNAPSHOT/lib -name 'tsr2*' \\ -o -name 'spark-r2*' -o -name '*jedis*' -o -name 'commons*' -o -name 'jdeferred*' -o -name 'geospark*' \\ -o -name 'gt-*' | tr '\\n' ',')\" Finally, start Zeppelin daemon. $ cd $ZEPPELIN_HOME/bin $ ./zeppelin-daemon.sh start","title":"1. Setting for Zeppelin"},{"location":"try-with-zeppelin/#2-tutorial-with-zeppelin","text":"After starting zeppelin daemon, you can access zeppelin UI using browser. The url is https://your-server-ip:8080 . There is a github page for tutorial . The repository includes a tool for generating sample data and a notebook for tutorial. You can import the tutorial notebook with its url. https://raw.githubusercontent.com/mnms/tutorials/master/zeppelin-notebook/note.json The tutorial runs on the spark interpreter of Zeppelin. Please make sure that the memory of Spark driver is at least 10GB in Spark interpreter setting. Also, make sure that the timeout of shell command is at least 120000 ms.","title":"2. Tutorial with Zeppelin"}]}